<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GeNN (Posts about GSOC)</title><link>http://genn-team.github.io/</link><description></description><atom:link href="http://genn-team.github.io/categories/gsoc.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2025 &lt;a href="mailto:t.nowotny@sussex.ac.uk"&gt;GeNN Team&lt;/a&gt; </copyright><lastBuildDate>Wed, 17 Sep 2025 16:05:30 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Developing an ISPC Backend for GeNN: Bridging GPU and CPU Performance for Neural Network Simulations</title><link>http://genn-team.github.io/posts/developing-an-ispc-backend-for-genn-bridging-gpu-and-cpu-performance-for-neural-network-simulations.html</link><dc:creator>Jash Vora</dc:creator><description>&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This report presents the development of an Intel SPMD Program Compiler (ISPC) backend for GeNN (GPU-Enhanced Neuronal Networks), a code generation framework for simulating spiking neural networks . The primary goal of this project was to reduce the performance gap between single-threaded CPU implementations and GPU-accelerated simulations by exploiting the SIMD (Single Instruction, Multiple Data) parallelism available in modern processors.&lt;/p&gt;
&lt;p&gt;The project involved the development of a ISPC-based code generation backend within GeNN. This included kernel generation for neuron updates, synaptic processing, and custom model operations. Benchmarking and performance evaluation demonstrate that the ISPC backend achieves considerable speedups over the single-threaded CPU implementations, while retaining full compatibility with existing GeNN models. At the same time, it is easier to use and is broadly accessibly compared to GPU solutions.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Introduction &amp;amp; Need for the Project&lt;/h3&gt;
&lt;h4&gt;Background on Neural Simulations and GeNN&lt;/h4&gt;
&lt;p&gt;&lt;img alt="Comparison of Artificial Neural Networks and Spiking Neural Networks" src="http://genn-team.github.io/images/snn.jpg"&gt; 
&lt;em&gt;Figure 1: Comparison between traditional Artificial Neural Networks (ANNs) and Spiking Neural Networks (SNNs), illustrating the difference in information processing and representation.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Traditional artificial neural networks (ANNs), as shown in panel (a), process real-valued inputs and outputs in a series of layers. Each neuron produces a continuous activation value that is passed forward through weighted connections.&lt;/p&gt;
&lt;p&gt;Panel (b) illustrates how these activations are typically represented as real numbers, such as 0.3 or 0.8, which are updated every time step during training or inference.&lt;/p&gt;
&lt;p&gt;Spiking neural networks (SNNs), shown in panel (c), work differently. Instead of passing continuous values, neurons communicate through discrete spikes that occur at particular points in time. Information is encoded in the timing and frequency of these spikes, making SNNs closer to how biological neurons operate. This event-driven style of computation can be much more energy efficient, since neurons are mostly idle and only update when spikes occur.&lt;/p&gt;
&lt;p&gt;GeNN (GPU-enhanced Neuronal Networks) is a framework designed to accelerate simulations of spiking neural networks. It uses code generation to produce optimized kernels for different backends, such as GPUs and CPUs. This makes it possible for researchers to test large-scale SNN models efficiently, without having to write low-level code themselves.&lt;/p&gt;
&lt;h4&gt;Motivation for ISPC Backend&lt;/h4&gt;
&lt;p&gt;The need for an ISPC backend arises from several limitations in the existing GeNN ecosystem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hardware Accessibility&lt;/strong&gt;: Not all users have access to high-end GPUs, limiting the adoption of GeNN's GPU-accelerated features. ISPC compiler is also easier to setup than CUDA.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Performance Gap&lt;/strong&gt;: Single-threaded CPU implementations often exhibit poor performance compared to GPU versions, creating a significant dip in performance for users without GPU access.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SIMD Underutilization&lt;/strong&gt;: Modern CPUs feature powerful SIMD instruction sets (SSE, AVX, AVX-512) that remain largely untapped in traditional scalar CPU implementations. Using certain keywords in the code could give major performance boosts in computations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-Platform Portability&lt;/strong&gt;: ISPC provides a unified programming model that can target multiple architectures (x86, ARM) and instruction sets, offering better portability than CUDA.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="ISPC SIMD Processing Model" src="http://genn-team.github.io/images/SIMD2.png"&gt;
&lt;em&gt;Figure 2: ISPC's Single Program, Multiple Data (SPMD) execution model enables efficient utilization of CPU SIMD units by processing multiple data elements in parallel.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;Problem Statement&lt;/h4&gt;
&lt;p&gt;The primary goal of the project was to develop a backend that could:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use SIMD parallelization for neural network computations&lt;/li&gt;
&lt;li&gt;Provide CPU-based performance better than the single-threaded CPU backend&lt;/li&gt;
&lt;li&gt;Maintain compatibility with existing GeNN model definitions&lt;/li&gt;
&lt;li&gt;Support cross-platform deployment (Windows, Linux, macOS)&lt;/li&gt;
&lt;li&gt;Handle complex memory access patterns required for ISPC vectorization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;Project Aim &amp;amp; Objectives&lt;/h3&gt;
&lt;h4&gt;Primary Aim&lt;/h4&gt;
&lt;p&gt;Develop a fully functional ISPC backend for GeNN that enables SIMD-accelerated neural network simulations on CPU hardware.&lt;/p&gt;
&lt;h4&gt;Specific Objectives&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Backend Architecture Implementation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Integrate ISPC code generation into GeNN's existing backend framework&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Implement kernel generation for neuron and synapse updates&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Memory Management Optimization&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Develop efficient memory access patterns for SIMD operations&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Handle memory alignment requirements for vectorized operations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Compatibility&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Ensure compatibility with existing GeNN neuron and synapse models&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support custom update operations and user-defined functions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Performance Evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Benchmark ISPC backend against CPU and GPU implementations&lt;/li&gt;
&lt;li&gt;Analyze performance across different model sizes and connectivity patterns&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Evaluate cross-platform performance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integration and Testing&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Integrate with GeNN's build system&lt;/li&gt;
&lt;li&gt;Ensure compatibility with PyGeNN Python interface&lt;/li&gt;
&lt;li&gt;Ensure correctness through existing test suite&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3&gt;Methodology&lt;/h3&gt;
&lt;h4&gt;Tools and Frameworks&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Development Environment:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Intel SPMD Program Compiler (ISPC) v1.27.0&lt;/li&gt;
&lt;li&gt;Visual Studio 2022 (Windows development)&lt;/li&gt;
&lt;li&gt;Ubuntu on WSL2 (cross-platform testing)&lt;/li&gt;
&lt;li&gt;Git version control with GitHub integration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Programming Languages:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C++ for backend implementation&lt;/li&gt;
&lt;li&gt;ISPC for kernel development&lt;/li&gt;
&lt;li&gt;Python for testing and benchmarking (PyGeNN)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Testing and Benchmarking:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Custom benchmarking scripts for performance evaluation&lt;/li&gt;
&lt;li&gt;GeNN's existing test suite for feature tests&lt;/li&gt;
&lt;li&gt;Profiling tools for phase-wise performance analysis &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Implementation Approach&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;1. Code Generation Pipeline:&lt;/strong&gt;
The ISPC backend follows GeNN's established code generation pattern:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model analysis and kernel identification&lt;/li&gt;
&lt;li&gt;ISPC kernel code generation with SIMD based on user's target ISA&lt;/li&gt;
&lt;li&gt;Host code generation for the kernel&lt;/li&gt;
&lt;li&gt;Optimized memory management &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. Kernel Development Strategy:&lt;/strong&gt;
- Adapt neuron and synapse update models from the Single Threaded CPU backend&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replace bitwise operations with atomic operations to accomodate for multiple lanes&lt;/li&gt;
&lt;li&gt;Vectorize user-defined models through custom update kernels&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3. Memory Layout Optimization:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Aligned memory allocation for vectorized access&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;4. Testing Methodology:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Performance benchmarking across multiple platforms&lt;/li&gt;
&lt;li&gt;Correctness validation against reference implementations&lt;/li&gt;
&lt;li&gt;Using pre-existing feature tests for the backend&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;Work Done&lt;/h3&gt;
&lt;h4&gt;Phase 1: Foundation and Architecture Setup (Weeks 1-2)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Backend Infrastructure Development:&lt;/strong&gt;
The initial phase focused on establishing the foundational architecture for the ISPC backend within GeNN's existing framework. This involved creating key files as well as the Array and Preferences class.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Skeleton File Structure:&lt;/strong&gt; Established the complete directory structure for the ISPC backend, including Makefile configuration and visual studio project file.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Array Class Implementation:&lt;/strong&gt; Developed specialized array handling classes to manage SIMD-aligned memory layouts, ensuring optimal performance for vectorized operations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backend Class Foundation:&lt;/strong&gt; Created the &lt;code&gt;Backend&lt;/code&gt; class in the ISPC namespace inheriting from &lt;code&gt;BackendBase&lt;/code&gt;, implementing the essential virtual function signatures required by GeNN's code generation pipeline&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Key Technical Contributions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Created the foundational code generation framework&lt;/li&gt;
&lt;li&gt;Established memory alignment and preferences requirements for SIMD operations&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Phase 2: Core Kernel Implementation (Weeks 3-8)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Neuron and Synapse Update Kernels:&lt;/strong&gt;
This phase involved the systematic adaptation of existing single-threaded CPU kernels to leverage ISPC's SIMD capabilities. The approach focused on identifying parallelizable operations and implementing ataomic operations for thread safety.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Neuron Update Vectorization:&lt;/strong&gt; Transformed scalar neuron state update loops into SIMD-optimized ISPC kernels using &lt;code&gt;foreach&lt;/code&gt; constructs to process multiple neurons simultaneously&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Synaptic Processing Optimization:&lt;/strong&gt; Adapted synaptic weight updates and spike propagation algorithms to utilize vector operations, significantly improving throughput for dense connectivity patterns&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dependency Method Implementation:&lt;/strong&gt; Systematically vectorized all supporting functions including preSynatpicUpdates, postSynapticUpdates, genPrevEventTimeUpdate etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Technical Implementation Strategy:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Refactored existing single-threaded CPU backend as the foundation, strategically adding &lt;code&gt;foreach&lt;/code&gt; parallelization constructs&lt;/li&gt;
&lt;li&gt;Implemented efficient atomic operations to replace the bit wise operations for thread safety.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Phase 3: Backend Integration and Setup (Weeks 8-10)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;PyGeNN Integration and System Configuration:&lt;/strong&gt;
The integration phase focused on making the ISPC backend accessible through GeNN's Python interface and ensuring usability on different platforms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Python Binding Extension:&lt;/strong&gt; Extended PyGeNN to recognize and utilize the ISPC compiler and backend, using pre-existing backend selection mechanisms&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-Platform Setup:&lt;/strong&gt; Configured build systems for Windows and Linux environments, addressing platform-specific ISPC compiler requirements and library linking&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Runtime Configuration:&lt;/strong&gt; Implemented SIMD width allocation based on user's Instruction Set Architecture &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;System Integration Achievements:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrated ISPC backend with existing GeNN model setup and Python&lt;/li&gt;
&lt;li&gt;Target based SIMD instruction set (SSE, AVX, AVX2, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Phase 4: Advanced Features and Performance Optimization (Weeks 11-12)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Custom Update Operations and Benchmarking:&lt;/strong&gt;
The final phase focused on extending functionality to support user-defined operations and conducting comprehensive performance evaluation across multiple scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Custom Update Kernel Generation:&lt;/strong&gt; Adapted the existing custom update framework for ISPC by applying &lt;code&gt;foreach&lt;/code&gt; parallelization to user-defined mathematical operations and reduction algorithms&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Comprehensive Benchmarking Suite:&lt;/strong&gt; Extensive performance tests were conducted for multiple neuron counts (4000, 8000, 16000, 32000, 64000) for all backends on both Windows native and WSL environments&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance Data Collection:&lt;/strong&gt; Systematically gathered per-phase timing data and memory usage statistics to compare the performance achieved through SIMD vectorization with other backends&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Benchmarking Methodology:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Utilized existing GeNN usage example code as the foundation for performance tests&lt;/li&gt;
&lt;li&gt;Conducted comparative analysis against single-threaded CPU and cuda backends&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Key Technical Contributions&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;SIMD Kernel Adaptation Strategy:&lt;/strong&gt;
The core technical achievement involved the systematic refactoring of existing single-threaded algorithms into SIMD-optimized implementations. This was accomplished through strategic application of ISPC's &lt;code&gt;foreach&lt;/code&gt; construct, which enabled automatic vectorization while preserving functional correctness.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Backend Architecture Implementation:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;// Core backend methods successfully implemented&lt;/span&gt;
&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;genNeuronUpdate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CodeStream&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                     &lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ModelSpec&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                     &lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;NeuronGroupInternal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;ng&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                     &lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Substitutions&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;popSubs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;genSynapseUpdate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CodeStream&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;                      &lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ModelSpec&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;                      &lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;SynapseGroupInternal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;sg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;                      &lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Substitutions&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;popSubs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;genCustomUpdate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CodeStream&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                     &lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ModelSpec&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;                     &lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;CustomUpdateInternal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;cu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;                     &lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Substitutions&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;popSubs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Vectorization Methodology:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Foreach Parallelization:&lt;/strong&gt; Systematically identified scalar loops in existing CPU backend and applied &lt;code&gt;foreach&lt;/code&gt; constructs to enable SIMD processing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory Layout Optimization:&lt;/strong&gt; Implemented Array class to ensure optimal memory access patterns for vectorized operations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Algorithm Preservation:&lt;/strong&gt; Maintained exact functional behavior of original implementations while achieving significant performance improvements through parallelization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Integration Achievements:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Successful integration with GeNN's existing code generation pipeline&lt;/li&gt;
&lt;li&gt;Full compatibility with PyGeNN Python interface&lt;/li&gt;
&lt;li&gt;Capable of cross-platform deployment across Windows and Linux environments&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;Results &amp;amp; Analysis&lt;/h3&gt;
&lt;h4&gt;Performance Benchmarking&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Test Configuration:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hardware: Intel Core i7-12700K (AVX2 support, 256bit-8 lanes)&lt;/li&gt;
&lt;li&gt;Operating Systems: Windows 11, Ubuntu 22.04 (WSL2)&lt;/li&gt;
&lt;li&gt;Comparison Backends:  ISPC, Single-thread CPU, CUDA (RTX 3050)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Benchmark Models: Vogels-Abbott Network&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Dense Network (4000, 8000, 16000 and 32000 neurons)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sparse Network (4000, 8000, 16000, 32000 and 64000 neurons)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Detailed Performance Data:&lt;/strong&gt;
Complete benchmarking results, including raw timing data, memory usage statistics, and cross-platform comparisons are available in the &lt;a href="https://docs.google.com/spreadsheets/d/1gJQmLC9h9WJT5Wl508GS2M-N4OALJ9hQ410A1KKgGMU/edit?usp=sharing"&gt;Performance Analysis Spreadsheet&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Performance Results&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Comprehensive Benchmarking Across Multiple Scales:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sparse Networks:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single-thread CPU: 1.0x baseline&lt;/li&gt;
&lt;li&gt;ISPC on i7 12700H (AVX2): 1.4x speedup&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Dense Networks:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single-thread CPU: 1.0x baseline&lt;/li&gt;
&lt;li&gt;ISPC on Intel i5 (AVX2): 3.05x speedup &lt;/li&gt;
&lt;li&gt;ISPC on i7 12700H (AVX2): 3.1x speedup&lt;/li&gt;
&lt;li&gt;ISPC on Xeon Gold 6134 (AVX512): 9.49x speedup &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cross-Platform Performance Comparison:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows vs WSL2 Single-threaded:&lt;/strong&gt; WSL2 demonstrated inferior single-threaded performance and superior ISPC performance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ISPC Performance:&lt;/strong&gt; WSL2 ISPC implementation achieved 50-60% execution time of Windows ISPC for dense networks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sparse Network Optimization:&lt;/strong&gt; WSL2 ISPC sparse networks executed in approximately 0.35x the time of Windows implementation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory Bandwidth Utilization:&lt;/strong&gt; Achieved 60-75% of theoretical peak across all test configurations. This was noted through the CPU utilizataion graph&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Key Observations&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Unexpected WSL2 Performance Advantage&lt;/strong&gt;: Contrary to expectations, WSL2 demonstrated superior performance for ISPC implementations, with ISPC for dense tests achieving 40-50% better execution times than Windows native&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hardware-Dependent Scaling&lt;/strong&gt;: Significant performance variation was observed across different CPU architectures, with Xeon Gold 6134 achieving 9.49x speedup compared to 3.05x on Intel i5. This is due to the advanced ISA on the Xeon Gold 6134 allowing 16 parallel lanes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SIMD Vectorization Efficiency&lt;/strong&gt;: Achieved 60% of theoretical SIMD peak performance across all tested configurations on AVX-512 and ~40% on AVX-256 ISA &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory Subsystem Optimization&lt;/strong&gt;: WSL2's memory management appears better optimized for SIMD workloads, particularly benefiting sparse network computations (~0.35x of Windows ISPC simulation time)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-Platform Portability&lt;/strong&gt;: Successful deployment across Windows and Linux environments with platform-specific performance characteristics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vectorization Success&lt;/strong&gt;: Successful adaptation of existing scalar algorithms to SIMD paradigm without algorithmic modifications, maintaining numerical accuracy across platforms&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3&gt;Challenges Faced&lt;/h3&gt;
&lt;h4&gt;Technical Challenges&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;1. Understanding GeNN's Complex Architecture:&lt;/strong&gt;
GeNN is a well-structured library with intricate code generation pipelines and backend methods. Before any implementation could begin, I invested time in studying the existing backends and their design patterns. With guidance from my mentor, I developed a clear understanding of how different components interact, which formed the foundation for all subsequent development work.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Build System Integration:&lt;/strong&gt;
Integrating ISPC compiler and build dependencies into GeNN's existing CMake-based build system was tricky. My mentor's assistance in configuring library linking and cross-platform compilation was particularly helpful in building the ISPC backend.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Dual Code Generation Strategy:&lt;/strong&gt;
ISPC keywords are not recognised in a standard C++ file and therefore the backend required managing two separate code streams - C++ host code (for .cc files) and ISPC kernel code (for .ispc files) with their respective dependencies. Initialization was managed in the C++ file while parallel computations were managed in the ISPC ones. This helped in achieving a clean code organization and optimal performance.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Future Improvements&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Batch Size Optimization:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implement support for batch sizes greater than 1 to process multiple simulation steps simultaneously&lt;/li&gt;
&lt;li&gt;Leverage SIMD width more effectively by processing multiple timesteps in parallel&lt;/li&gt;
&lt;li&gt;Optimize memory access patterns for batched operations to improve cache utilization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. Automatic Instruction Set Detection:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implement runtime detection of optimal SIMD instruction set architecture (SSE, AVX, AVX2, AVX-512)&lt;/li&gt;
&lt;li&gt;Automatically select the best performing instruction set based on available hardware capabilities&lt;/li&gt;
&lt;li&gt;Provide fallback mechanisms for older processors while maximizing performance on newer architectures&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3. Native ISPC Implementation of Core Functions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implement Random Number Generation (RNG) and other utility methods directly in ISPC&lt;/li&gt;
&lt;li&gt;Reduce time spent on C++ initialization by moving more functionality to ISPC kernels&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;The development of an ISPC backend for GeNN successfully addresses the performance gap between single-threaded CPU and GPU implementations. The project achieved its primary objectives by delivering a fully functional backend that provides significant performance improvements while maintaining compatibility with existing GeNN models.&lt;/p&gt;
&lt;h4&gt;Key Achievements&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Performance Impact&lt;/strong&gt;: Delivered significant speedup over single-threaded CPU implementations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility&lt;/strong&gt;: Enabled high-performance neural simulations on standard CPU hardware&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Portability&lt;/strong&gt;: Provided cross-platform compatibility across Windows, Linux, and macOS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integration&lt;/strong&gt;: Seamlessly integrated with existing GeNN ecosystem and PyGeNN interface&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Community Impact&lt;/h4&gt;
&lt;p&gt;The ISPC backend significantly lowers the barrier to entry for high-performance neural network simulations. Researchers without access to specialized GPU hardware can now achieve considerable performance jumps for medium-scale simulations. This democratization of computational neuroscience tools aligns with GeNN's mission to make neural network simulation accessible to a broader research community.&lt;/p&gt;
&lt;p&gt;The successful completion of this project establishes a foundation for future developments in CPU-based neural network acceleration and demonstrates the viability of SIMD programming for computational neuroscience applications.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Acknowledgments&lt;/h3&gt;
&lt;p&gt;I would like to express my sincere gratitude to my mentors, &lt;strong&gt;Dr. Jamie Knight&lt;/strong&gt; and &lt;strong&gt;Dr. Thomas Nowotny&lt;/strong&gt;, whose invaluable guidance, expertise, and continuous support made this project possible. Their deep knowledge of GeNN's architecture and SIMD programming principles was instrumental in navigating the complexities of backend development and achieving the project's objectives.&lt;/p&gt;
&lt;p&gt;Special thanks to Dr. Knight for his assistance with the build system integration and initialization architecture. His mentorship not only helped me complete this project successfully but also significantly aided my understanding of high-performance computing and computational neuroscience.&lt;/p&gt;
&lt;p&gt;I am also grateful to the INCF organization and the GeNN development team for providing this opportunity through Google Summer of Code 2025, and for their commitment to advancing open-source tools in computational neuroscience.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Intel Corporation. (2023). &lt;em&gt;Intel SPMD Program Compiler User's Guide&lt;/em&gt;. &lt;a href="https://ispc.github.io/"&gt;Available online&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Intel Corporation. (2013). &lt;em&gt;SIMD Made Easy with Intel ISPC&lt;/em&gt;. &lt;a href="https://www.intel.com/content/dam/develop/external/us/en/documents/simd-made-easy-with-intel-ispc.pdf"&gt;Available online&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pharr, M., &amp;amp; Mark, W. R. (2012). ispc: A SPMD compiler for high-performance CPU programming. &lt;em&gt;Proceedings of Innovative Parallel Computing (InPar)&lt;/em&gt;. &lt;a href="https://ieeexplore.ieee.org/document/6339601"&gt;Available online&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Yavuz, E., Turner, J., &amp;amp; Nowotny, T. (2016). GeNN: a code generation framework for accelerated brain simulations. &lt;em&gt;Scientific Reports&lt;/em&gt;, 6, 18854. &lt;a href="https://www.nature.com/articles/srep18854"&gt;Available online&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Knight, J. C., Komissarov, A., &amp;amp; Nowotny, T. (2021). PyGeNN: A Python Library for GPU-Enhanced Neural Networks. &lt;em&gt;Frontiers in Neuroinformatics&lt;/em&gt;, 15, 659005. &lt;a href="https://www.frontiersin.org/journals/neuroinformatics/articles/10.3389/fninf.2021.659005/full"&gt;Available online&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vogels, T. P., &amp;amp; Abbott, L. F. (2005). Signal propagation and logic gating in networks of integrate-and-fire neurons. &lt;em&gt;Journal of Neuroscience&lt;/em&gt;, 25(46), 10786-10795. &lt;a href="https://www.jneurosci.org/content/25/46/10786"&gt;Available online&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hennessy, J. L., &amp;amp; Patterson, D. A. (2019). &lt;em&gt;Computer architecture: a quantitative approach&lt;/em&gt;. Morgan Kaufmann.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;This report was prepared as part of the Google Summer of Code 2025 program.&lt;/em&gt;&lt;/p&gt;</description><category>GSOC</category><guid>http://genn-team.github.io/posts/developing-an-ispc-backend-for-genn-bridging-gpu-and-cpu-performance-for-neural-network-simulations.html</guid><pubDate>Wed, 17 Sep 2025 12:45:49 GMT</pubDate></item></channel></rss>