<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GeNN (Posts by Jamie)</title><link>http://genn-team.github.io/</link><description></description><atom:link href="http://genn-team.github.io/authors/jamie.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2025 &lt;a href="mailto:t.nowotny@sussex.ac.uk"&gt;GeNN Team&lt;/a&gt; </copyright><lastBuildDate>Wed, 17 Sep 2025 16:30:03 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Software Developer Blog: Running away</title><link>http://genn-team.github.io/posts/running-away.html</link><dc:creator>Jamie</dc:creator><description>&lt;p&gt;After spending a long time updating GeNN's code generator to generate more efficient CUDA kernels which have the side benefit of compiling much more quickly, there remained something of a dirty secret.
The &lt;code&gt;runner.cc&lt;/code&gt; file which contains the helper functions generated by GeNN for allocating memory and copying variables between GPU and CPU could still easily grow to the point that compilation would take an extremely long time and consume all available memory.
For our &lt;a href="https://github.com/neworderofjamie/multi-area-model/"&gt;multi-area model implementation&lt;/a&gt;, I added various options which turn off the generation of empty functions and, as everything in this model was generated on the GPU anyway, I also turned off the generation of host copies of almost all variables.
This resulted in a paltry 40 mbyte &lt;code&gt;runner.cc&lt;/code&gt; which compiled in a couple of minutes which, for a model this size, is just about acceptable.
However, as users have started making bigger models and not always wanting to generate everything on the GPU, this issue has kept reappearing.&lt;/p&gt;
&lt;h2&gt;Jinjaly investigating&lt;/h2&gt;
&lt;p&gt;To investigate this in a slightly simpler way than just building larger and larger GeNN models until things break, I used &lt;a href="https://jinja.palletsprojects.com"&gt;Jinja&lt;/a&gt; to build a template that could generate fake &lt;code&gt;runner.cc&lt;/code&gt; files containing varying number of arrays, representing the state variables in a real model. 
The heart of this template looked something like this:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;// Push and pull functions&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;arrays&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;push&lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;}}&lt;/span&gt;&lt;span class="n"&gt;ToDevice&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;CHECK_CUDA_ERRORS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cudaMemcpy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d_&lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;}},&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;}},&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                 &lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;}}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                 &lt;/span&gt;&lt;span class="n"&gt;cudaMemcpyHostToDevice&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pull&lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;}}&lt;/span&gt;&lt;span class="n"&gt;FromDevice&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;CHECK_CUDA_ERRORS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cudaMemcpy&lt;/span&gt;&lt;span class="p"&gt;({{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;}},&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;d_&lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;}},&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                 &lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;}}&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                 &lt;/span&gt;&lt;span class="n"&gt;cudaMemcpyDeviceToHost&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;endfor&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;allocateMem&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;CHECK_CUDA_ERRORS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cudaSetDevice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;arrays&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;CHECK_CUDA_ERRORS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cudaHostAlloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;}},&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                    &lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;}}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                    &lt;/span&gt;&lt;span class="n"&gt;cudaHostAllocPortable&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;CHECK_CUDA_ERRORS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cudaMalloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;d_&lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;}},&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;                                 &lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;}}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)));&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;endfor&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;    &lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;this template (saved in &lt;code&gt;runner.cc.template&lt;/code&gt;) could then be used to generate C++ and print it to stdout like:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;jinja2&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Template&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"runner.cc.template"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"r"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;template&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Template&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="n"&gt;arrays&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;&lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"array_&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"size"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; 
          &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_arrays&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;


&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;template&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;render&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arrays&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;arrays&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On Linux, the C++ could then be built using the same command line used by GeNN itself (some options omitted for brevity) and timed using the &lt;code&gt;/usr/bin/time&lt;/code&gt; (I recently discovered that &lt;code&gt;command time&lt;/code&gt; can be used as an alternative way of disambiguating this from the bash builtin &lt;code&gt;time&lt;/code&gt;):&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;/usr/bin/time&lt;span class="w"&gt; &lt;/span&gt;-v&lt;span class="w"&gt; &lt;/span&gt;nvcc&lt;span class="w"&gt; &lt;/span&gt;-c&lt;span class="w"&gt; &lt;/span&gt;-x&lt;span class="w"&gt; &lt;/span&gt;cu&lt;span class="w"&gt; &lt;/span&gt;-arch&lt;span class="w"&gt; &lt;/span&gt;sm_86&lt;span class="w"&gt; &lt;/span&gt;-std&lt;span class="o"&gt;=&lt;/span&gt;c++11&lt;span class="w"&gt; &lt;/span&gt;test.cc
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Initial experiments showed that while both the wall clock time and maximum resident set size (roughly analagous to peak physical memory usage) grow approximately linearly (much to my relief after reading way too many &lt;a href="https://randomascii.wordpress.com/category/quadratic/"&gt;horror stories&lt;/a&gt; on Bruce Dawson's &lt;a href="https://randomascii.wordpress.com"&gt;excellent blog&lt;/a&gt;) with the number of arrays, it still grew extremely rapidly:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Relationship between runner size; and compile time and memory usage" src="http://genn-team.github.io/images/blog_running_away/fig_1_linearity.png"&gt;&lt;/p&gt;
&lt;p&gt;Therefore, a model with 10000 arrays will take over 4 minutes and around 8 gbyte of memory to compile â neither of which are really acceptable.
To put this in perspective, if you split a model up into about 100 populations and connect most of the permutations together (this is an all-too-reasonable assumption in many areas of the mammalian brain), you could easily reach this many variables.&lt;/p&gt;
&lt;p&gt;So....what is NVCC doing with all this time and memory?
&lt;code&gt;runner.cc&lt;/code&gt; only contains host code (NVCC is just used to ensure the same compiler/options across execution units and to deal with setting up the CUDA linker/include paths) but, when you pass a 5 mbyte &lt;code&gt;runner.cc&lt;/code&gt; file to NVCC, the file that is passed on to the host compiler (GCC) has grown to 15 mbyte!
However, this turned out to be simply because NVCC is in charge of running the preprocessor so that 10 mbyte is 'just' the result of expanding macros and including C++ standard library header files!&lt;/p&gt;
&lt;h2&gt;Profiling&lt;/h2&gt;
&lt;p&gt;Around this point, I remembered reading a blog post about &lt;a href="https://aras-p.info/blog/2019/01/12/Investigating-compile-times-and-Clang-ftime-report/"&gt;profiling compiler times&lt;/a&gt; on yet another &lt;a href="https://aras-p.info/blog/"&gt;excellent blog&lt;/a&gt; and turned on the &lt;code&gt;-ftime-report&lt;/code&gt; GCC option.
As the blog promised, this generates a gargantuan report which starts by splitting the compilation time of this 10000 array model into 'phases':&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;phase setup             :  ...   0.00 ( 0%) wall    1384 kB ( 0%)
phase parsing           :  ...  20.81 ( 9%) wall 1794944 kB (25%)
phase lang. deferred    :  ...   0.02 ( 0%) wall    2426 kB ( 0%)
phase opt and generate  :  ... 214.14 (91%) wall 5412439 kB (75%)
phase finalize          :  ...   0.54 ( 0%) wall       0 kB ( 0%)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Somewhat surprisingly (as, after all, we're throwing a massive source file at GCC), the vast majority of time is spent in "opt and generate" (code-generation and optimisation) rather than in parsing.
Looking a little further down, where the report contains a seeminly unsorted list of processes within phases, the only other 'hot' line is:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;expand vars             :  ... 101.93 (43%) wall   50597 kB ( 1%)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;but, as a non-GCC developer, this doesn't help me a great deal....back to pursuing random hunches!&lt;/p&gt;
&lt;h2&gt;Smoking gun&lt;/h2&gt;
&lt;p&gt;As there's no virtual functions in this code, my C++ prejudices suggest that only exceptions could &lt;em&gt;possibly&lt;/em&gt; be to blame and, as each of those &lt;code&gt;CHECK_CUDA_ERRORS&lt;/code&gt; macros hides a &lt;code&gt;throw std::runtime_error&lt;/code&gt;, maybe that's not unreasonable.
Generating all that zero-cost abstraction must involve &lt;em&gt;expanding&lt;/em&gt; a lot of &lt;em&gt;variables&lt;/em&gt;....right!?
How about if we replace our current implemementation of &lt;code&gt;CHECK_CUDA_ERRORS&lt;/code&gt;:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="cp"&gt;#define CHECK_CUDA_ERRORS(call) {\&lt;/span&gt;
&lt;span class="cp"&gt;    cudaError_t error = call;\&lt;/span&gt;
&lt;span class="cp"&gt;    if(error != cudaSuccess) {\&lt;/span&gt;
&lt;span class="cp"&gt;        throw std::runtime_error(__FILE__": " \&lt;/span&gt;
&lt;span class="cp"&gt;                                 + std::to_string(__LINE__) \&lt;/span&gt;
&lt;span class="cp"&gt;                                 + ": cuda error " \&lt;/span&gt;
&lt;span class="cp"&gt;                                 + std::to_string(error) \&lt;/span&gt;
&lt;span class="cp"&gt;                                 + ": " + cudaGetErrorString(error));\&lt;/span&gt;
&lt;span class="cp"&gt;    }\&lt;/span&gt;
&lt;span class="cp"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;with:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="cp"&gt;#define CHECK_CUDA_ERRORS(call) {\&lt;/span&gt;
&lt;span class="cp"&gt;    cudaError_t error = call;\&lt;/span&gt;
&lt;span class="cp"&gt;    assert(error == cudaSuccess); \&lt;/span&gt;
&lt;span class="cp"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;or even:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="cp"&gt;#define CHECK_CUDA_ERRORS(call) {\&lt;/span&gt;
&lt;span class="cp"&gt;    cudaError_t error = call;\&lt;/span&gt;
&lt;span class="cp"&gt;    if(error != cudaSuccess) {\&lt;/span&gt;
&lt;span class="cp"&gt;        std::abort();\&lt;/span&gt;
&lt;span class="cp"&gt;    }\&lt;/span&gt;
&lt;span class="cp"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Some template-meddling and sweeping later we can produce:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Relationship between number of arrays and error handling approach; and compile time and memory usage" src="http://genn-team.github.io/images/blog_running_away/fig_2_alternate_error.png"&gt;&lt;/p&gt;
&lt;p&gt;Seems like this actually works! Our 10000 array model now only takes 30 seconds and less than 1 gbyte of memory to compile which is much more reasonable!&lt;/p&gt;
&lt;p&gt;However, are the exceptions &lt;em&gt;really&lt;/em&gt; to blame? 
Compile times seem much better when using the single-threaded CPU backend and that allocates memory with &lt;code&gt;new[]&lt;/code&gt; (obviously, modern C++ rules don't apply in generated code...) which throws &lt;code&gt;std::bad_alloc&lt;/code&gt; to signal failure.
Admittedly, because there's no need to copy data when everthing's on the CPU, this backend generates empty 'push' and 'pull' functions so there's less code to compile overall but, if generating exception handling code was the problem, you would expect issues here too.
Maybe expanding all that message-generating code is the real issue...
How about we hack the following additional variants into the template:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="cp"&gt;#define CHECK_CUDA_ERRORS(call) {\&lt;/span&gt;
&lt;span class="cp"&gt;    cudaError_t error = call;\&lt;/span&gt;
&lt;span class="cp"&gt;    if(error != cudaSuccess) {\&lt;/span&gt;
&lt;span class="cp"&gt;        throw std::runtime_error();\&lt;/span&gt;
&lt;span class="cp"&gt;    }\&lt;/span&gt;
&lt;span class="cp"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="cp"&gt;#define CHECK_CUDA_ERRORS(call) {\&lt;/span&gt;
&lt;span class="cp"&gt;    cudaError_t error = call;\&lt;/span&gt;
&lt;span class="cp"&gt;    if(error != cudaSuccess) {\&lt;/span&gt;
&lt;span class="cp"&gt;        std::cerr &amp;lt;&amp;lt; __FILE__ &amp;lt;&amp;lt; ": " &amp;lt;&amp;lt; __LINE__;\&lt;/span&gt;
&lt;span class="cp"&gt;        std::cerr &amp;lt;&amp;lt; ": cuda error " &amp;lt;&amp;lt; error &amp;lt;&amp;lt; ": ";\&lt;/span&gt;
&lt;span class="cp"&gt;        std::cerr &amp;lt;&amp;lt; cudaGetErrorString(error) &amp;lt;&amp;lt; std::endl;\&lt;/span&gt;
&lt;span class="cp"&gt;        std::abort();\&lt;/span&gt;
&lt;span class="cp"&gt;    }\&lt;/span&gt;
&lt;span class="cp"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and sweep:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Relationship between number of arrays and error handling approach; and compile time and memory usage" src="http://genn-team.github.io/images/blog_running_away/fig_3_more_alternate_error.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;std::abort&lt;/code&gt; is definitely easier on the compiler than throwing exceptions but, compiling the message generation code also seems to make a large difference.&lt;/p&gt;
&lt;h2&gt;MSVC&lt;/h2&gt;
&lt;p&gt;The other compiler I often use with GeNN is Microsoft Visual C++.
I can't quite face repeating this whole process again but, initial tests suggest that this optimisation is even more valuable here.
Using the simplest &lt;code&gt;std::abort&lt;/code&gt; raising &lt;code&gt;CHECK_CUDA_ERRORS&lt;/code&gt; macro, the 10000 array model can be compiled in around 19 seconds whereas, using the original exception-throwing macro...I have given up waiting after around 1 hour!&lt;/p&gt;
&lt;h2&gt;Implementing a workaround&lt;/h2&gt;
&lt;p&gt;GeNN is approaching the end of the 4.X release cycle so, for now, I have added a simple but slightly hacky workaround for these issues by adding a &lt;code&gt;generateSimpleErrorHandling&lt;/code&gt; flag to GeNN's CUDA backend to switch from generating code with the previous full-fat &lt;code&gt;CHECK_CUDA_ERRORS&lt;/code&gt; macro to the simplest version which simply calls &lt;code&gt;std::abort&lt;/code&gt; without generating a message. This can be turned on from C++ like:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;modelDefinition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NNmodel&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;GENN_PREFERENCES&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generateSimpleErrorHandling&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;or from Python like:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GeNNModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"float"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"my_model"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;generateSimpleErrorHandling&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Real models&lt;/h2&gt;
&lt;p&gt;The largest model we currently have to play with with is the &lt;a href="https://github.com/neworderofjamie/multi-area-model/"&gt;multi-area cortical model&lt;/a&gt;.
Although it has 64516 synapse groups, due to its use of procedural connectivity (where all synaptic connectivity, weights and delays are generated on the fly), it doesn't actually have any per-synapse group variables with push and pull functions.
Nonetheless, using the new simple error handling reduces the compilation time of the &lt;code&gt;runner.cc&lt;/code&gt; from 155 to 129 seconds.
Finally, although it is not possible to &lt;em&gt;run&lt;/em&gt; the model in this way as no single GPU has enough memory, we can generate a &lt;code&gt;runner.cc&lt;/code&gt; from this model with standard, in-memory sparse connectivity and push and pull functions for each variable.
This results in a nightmarish, 114 mbyte &lt;code&gt;runner.cc&lt;/code&gt; which, using the original &lt;code&gt;CHECK_CUDA_ERRORS&lt;/code&gt; macro, would definitely be impossible to compile on any reasonable machine.
However, using the new simplified macro, the runner can be compiled in just over 20 minutes and requires just over 14 gbyte of memory â still pretty unusable but definitely progress!&lt;/p&gt;
&lt;h2&gt;Long-term solutions&lt;/h2&gt;
&lt;p&gt;The majority of the time, the errors which the &lt;code&gt;CHECK_CUDA_ERRORS&lt;/code&gt; macro is aiming to catch are out of memory errors in the &lt;code&gt;allocateMem&lt;/code&gt; function and errors that occured during (asynchronous) kernel launches that are only caught at the next push or pull call (which are typically the main synchronisation points) so perhaps, in future, we could adopt a more targetted error-handling approach which provides a balance between sufficient debugging information and compilation time.&lt;/p&gt;
&lt;p&gt;However, while the error handling changes discussed here allow the current approach to generating &lt;code&gt;runner.cc&lt;/code&gt; files to scale a bit further, the code we are generating is still pretty pathological, least of all because &lt;a href="https://github.com/genn-team/genn/issues/408"&gt;the Windows PE executable format has a limit of 65535 symbol limit&lt;/a&gt; which you can hit quite easily with a large model.
Early this year, I made an attempt at re-writing the code generator to apply the same merging strategy GeNN uses elsewhere to &lt;code&gt;runner.cc&lt;/code&gt;. 
This means that all the variables associated with neuron and synapse populations with the same types of state variable can be allocated using one piece of shared generated code.
While this works, it adds yet more complexity to GeNN and fundamentally breaks the 'classic' way of using GeNN from C++, where you link some C++ simulation code against your generated code and can access state variables directly by name.
However, based on this investigation, maybe that project needs resurrecting!&lt;/p&gt;
&lt;p&gt;All the code I've developed to explore this problem is available from &lt;a href="https://github.com/neworderofjamie/nvcc_breaker"&gt;my Github&lt;/a&gt;.&lt;/p&gt;</description><guid>http://genn-team.github.io/posts/running-away.html</guid><pubDate>Tue, 01 Nov 2022 13:35:07 GMT</pubDate></item></channel></rss>