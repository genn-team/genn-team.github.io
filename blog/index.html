<!DOCTYPE html>
<html prefix="
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Welcome to GeNN">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>GeNN · Software Developer Blog </title>
<link href="../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../rss.xml">
<link rel="canonical" href="http://genn-team.github.io/blog/">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]-->
</head>
<body class="">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="hsidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="http://genn-team.github.io/">
                      <h1 id="brand"><a href="http://genn-team.github.io/" title="GeNN" rel="home">

        <span class="blog-title" id="blog-title">GeNN</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead">Welcome to GeNN</p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="https://genn-team.github.io/genn/documentation/5/index.html">Documentation</a>
        <a class="sidebar-nav-item" href="../tutorials.html">Tutorials</a>
        <a class="sidebar-nav-item" href=".">Latest Blog</a>
        <a class="sidebar-nav-item" href="../archive.html">Blog Archive</a>
        <a class="sidebar-nav-item" href="../categories/">Blog Tags</a>
        <a class="sidebar-nav-item" href="../rss.xml">Blog RSS feed</a>
    
    
    </nav><footer id="footer"><span class="copyright">
              Contents © 2025         <a href="mailto:t.nowotny@sussex.ac.uk">GeNN Team</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            </span>
            
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
    
<div class="post">
    <article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="../posts/developing-an-ispc-backend-for-genn-bridging-gpu-and-cpu-performance-for-neural-network-simulations.html" class="u-url">Developing an ISPC Backend for GeNN: Bridging GPU and CPU Performance for Neural Network Simulations</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2025-09-17T13:45:49+01:00" title="2025-09-17 13:45">2025-09-17 13:45</time></span>
        </div>
    </header><div class="e-content entry-content">
    <h3>Abstract</h3>
<p>This report presents the development of an Intel SPMD Program Compiler (ISPC) backend for GeNN (GPU-Enhanced Neuronal Networks), a code generation framework for simulating spiking neural networks . The primary goal of this project was to reduce the performance gap between single-threaded CPU implementations and GPU-accelerated simulations by exploiting the SIMD (Single Instruction, Multiple Data) parallelism available in modern processors.</p>
<p>The project involved the development of a ISPC-based code generation backend within GeNN. This included kernel generation for neuron updates, synaptic processing, and custom model operations. Benchmarking and performance evaluation demonstrate that the ISPC backend achieves considerable speedups over the single-threaded CPU implementations, while retaining full compatibility with existing GeNN models. At the same time, it is easier to use and is broadly accessibly compared to GPU solutions.</p>
<hr>
<h3>Introduction &amp; Need for the Project</h3>
<h4>Background on Neural Simulations and GeNN</h4>
<p><img alt="Comparison of Artificial Neural Networks and Spiking Neural Networks" src="../images/snn.jpg"><em>Figure 1: Comparison between traditional Artificial Neural Networks (ANNs) and Spiking Neural Networks (SNNs), illustrating the difference in information processing and representation.</em></p>
<p>Traditional artificial neural networks (ANNs), as shown in panel (a), process real-valued inputs and outputs in a series of layers. Each neuron produces a continuous activation value that is passed forward through weighted connections.</p>
<p>Panel (b) illustrates how these activations are typically represented as real numbers, such as 0.3 or 0.8, which are updated every time step during training or inference.</p>
<p>Spiking neural networks (SNNs), shown in panel (c), work differently. Instead of passing continuous values, neurons communicate through discrete spikes that occur at particular points in time. Information is encoded in the timing and frequency of these spikes, making SNNs closer to how biological neurons operate. This event-driven style of computation can be much more energy efficient, since neurons are mostly idle and only update when spikes occur.</p>
<p>GeNN (GPU-enhanced Neuronal Networks) is a framework designed to accelerate simulations of spiking neural networks. It uses code generation to produce optimized kernels for different backends, such as GPUs and CPUs. This makes it possible for researchers to test large-scale SNN models efficiently, without having to write low-level code themselves.</p>
<h4>Motivation for ISPC Backend</h4>
<p>The need for an ISPC backend arises from several limitations in the existing GeNN ecosystem:</p>
<ol>
<li>
<p><strong>Hardware Accessibility</strong>: Not all users have access to high-end GPUs, limiting the adoption of GeNN's GPU-accelerated features. ISPC compiler is also easier to setup than CUDA.</p>
</li>
<li>
<p><strong>Performance Gap</strong>: Single-threaded CPU implementations often exhibit poor performance compared to GPU versions, creating a significant dip in performance for users without GPU access.</p>
</li>
<li>
<p><strong>SIMD Underutilization</strong>: Modern CPUs feature powerful SIMD instruction sets (SSE, AVX, AVX-512) that remain largely untapped in traditional scalar CPU implementations. Using certain keywords in the code could give major performance boosts in computations.</p>
</li>
<li>
<p><strong>Cross-Platform Portability</strong>: ISPC provides a unified programming model that can target multiple architectures (x86, ARM) and instruction sets, offering better portability than CUDA.</p>
</li>
</ol>
<p><img alt="ISPC SIMD Processing Model" src="../images/SIMD2.png"><em>Figure 2: ISPC's Single Program, Multiple Data (SPMD) execution model enables efficient utilization of CPU SIMD units by processing multiple data elements in parallel.</em></p>
<h4>Problem Statement</h4>
<p>The primary goal of the project was to develop a backend that could:</p>
<ul>
<li>Use SIMD parallelization for neural network computations</li>
<li>Provide CPU-based performance better than the single-threaded CPU backend</li>
<li>Maintain compatibility with existing GeNN model definitions</li>
<li>Support cross-platform deployment (Windows, Linux, macOS)</li>
<li>Handle complex memory access patterns required for ISPC vectorization</li>
</ul>
<hr>
<h3>Project Aim &amp; Objectives</h3>
<h4>Primary Aim</h4>
<p>Develop a fully functional ISPC backend for GeNN that enables SIMD-accelerated neural network simulations on CPU hardware.</p>
<h4>Specific Objectives</h4>
<ol>
<li><strong>Backend Architecture Implementation</strong></li>
<li>Integrate ISPC code generation into GeNN's existing backend framework</li>
<li>
<p>Implement kernel generation for neuron and synapse updates</p>
</li>
<li>
<p><strong>Memory Management Optimization</strong></p>
</li>
<li>Develop efficient memory access patterns for SIMD operations</li>
<li>
<p>Handle memory alignment requirements for vectorized operations</p>
</li>
<li>
<p><strong>Feature Compatibility</strong></p>
</li>
<li>Ensure compatibility with existing GeNN neuron and synapse models</li>
<li>
<p>Support custom update operations and user-defined functions</p>
</li>
<li>
<p><strong>Performance Evaluation</strong></p>
</li>
<li>Benchmark ISPC backend against CPU and GPU implementations</li>
<li>Analyze performance across different model sizes and connectivity patterns</li>
<li>
<p>Evaluate cross-platform performance</p>
</li>
<li>
<p><strong>Integration and Testing</strong></p>
</li>
<li>Integrate with GeNN's build system</li>
<li>Ensure compatibility with PyGeNN Python interface</li>
<li>Ensure correctness through existing test suite</li>
</ol>
<hr>
<h3>Methodology</h3>
<h4>Tools and Frameworks</h4>
<p><strong>Development Environment:</strong></p>
<ul>
<li>Intel SPMD Program Compiler (ISPC) v1.27.0</li>
<li>Visual Studio 2022 (Windows development)</li>
<li>Ubuntu on WSL2 (cross-platform testing)</li>
<li>Git version control with GitHub integration</li>
</ul>
<p><strong>Programming Languages:</strong></p>
<ul>
<li>C++ for backend implementation</li>
<li>ISPC for kernel development</li>
<li>Python for testing and benchmarking (PyGeNN)</li>
</ul>
<p><strong>Testing and Benchmarking:</strong></p>
<ul>
<li>Custom benchmarking scripts for performance evaluation</li>
<li>GeNN's existing test suite for feature tests</li>
<li>Profiling tools for phase-wise performance analysis </li>
</ul>
<h4>Implementation Approach</h4>
<p><strong>1. Code Generation Pipeline:</strong>
The ISPC backend follows GeNN's established code generation pattern:</p>
<ul>
<li>Model analysis and kernel identification</li>
<li>ISPC kernel code generation with SIMD based on user's target ISA</li>
<li>Host code generation for the kernel</li>
<li>Optimized memory management </li>
</ul>
<p><strong>2. Kernel Development Strategy:</strong>
- Adapt neuron and synapse update models from the Single Threaded CPU backend</p>
<ul>
<li>Replace bitwise operations with atomic operations to accomodate for multiple lanes</li>
<li>Vectorize user-defined models through custom update kernels</li>
</ul>
<p><strong>3. Memory Layout Optimization:</strong></p>
<ul>
<li>Aligned memory allocation for vectorized access</li>
</ul>
<p><strong>4. Testing Methodology:</strong></p>
<ul>
<li>Performance benchmarking across multiple platforms</li>
<li>Correctness validation against reference implementations</li>
<li>Using pre-existing feature tests for the backend</li>
</ul>
<hr>
<h3>Work Done</h3>
<h4>Phase 1: Foundation and Architecture Setup (Weeks 1-2)</h4>
<p><strong>Backend Infrastructure Development:</strong>
The initial phase focused on establishing the foundational architecture for the ISPC backend within GeNN's existing framework. This involved creating key files as well as the Array and Preferences class.</p>
<ul>
<li>
<strong>Skeleton File Structure:</strong> Established the complete directory structure for the ISPC backend, including Makefile configuration and visual studio project file.</li>
<li>
<strong>Array Class Implementation:</strong> Developed specialized array handling classes to manage SIMD-aligned memory layouts, ensuring optimal performance for vectorized operations</li>
<li>
<strong>Backend Class Foundation:</strong> Created the <code>Backend</code> class in the ISPC namespace inheriting from <code>BackendBase</code>, implementing the essential virtual function signatures required by GeNN's code generation pipeline</li>
</ul>
<p><strong>Key Technical Contributions:</strong></p>
<ul>
<li>Created the foundational code generation framework</li>
<li>Established memory alignment and preferences requirements for SIMD operations</li>
</ul>
<h4>Phase 2: Core Kernel Implementation (Weeks 3-8)</h4>
<p><strong>Neuron and Synapse Update Kernels:</strong>
This phase involved the systematic adaptation of existing single-threaded CPU kernels to leverage ISPC's SIMD capabilities. The approach focused on identifying parallelizable operations and implementing ataomic operations for thread safety.</p>
<ul>
<li>
<strong>Neuron Update Vectorization:</strong> Transformed scalar neuron state update loops into SIMD-optimized ISPC kernels using <code>foreach</code> constructs to process multiple neurons simultaneously</li>
<li>
<strong>Synaptic Processing Optimization:</strong> Adapted synaptic weight updates and spike propagation algorithms to utilize vector operations, significantly improving throughput for dense connectivity patterns</li>
<li>
<strong>Dependency Method Implementation:</strong> Systematically vectorized all supporting functions including preSynatpicUpdates, postSynapticUpdates, genPrevEventTimeUpdate etc.</li>
</ul>
<p><strong>Technical Implementation Strategy:</strong></p>
<ul>
<li>Refactored existing single-threaded CPU backend as the foundation, strategically adding <code>foreach</code> parallelization constructs</li>
<li>Implemented efficient atomic operations to replace the bit wise operations for thread safety.</li>
</ul>
<h4>Phase 3: Backend Integration and Setup (Weeks 8-10)</h4>
<p><strong>PyGeNN Integration and System Configuration:</strong>
The integration phase focused on making the ISPC backend accessible through GeNN's Python interface and ensuring usability on different platforms.</p>
<ul>
<li>
<strong>Python Binding Extension:</strong> Extended PyGeNN to recognize and utilize the ISPC compiler and backend, using pre-existing backend selection mechanisms</li>
<li>
<strong>Cross-Platform Setup:</strong> Configured build systems for Windows and Linux environments, addressing platform-specific ISPC compiler requirements and library linking</li>
<li>
<strong>Runtime Configuration:</strong> Implemented SIMD width allocation based on user's Instruction Set Architecture </li>
</ul>
<p><strong>System Integration Achievements:</strong></p>
<ul>
<li>Integrated ISPC backend with existing GeNN model setup and Python</li>
<li>Target based SIMD instruction set (SSE, AVX, AVX2, etc.)</li>
</ul>
<h4>Phase 4: Advanced Features and Performance Optimization (Weeks 11-12)</h4>
<p><strong>Custom Update Operations and Benchmarking:</strong>
The final phase focused on extending functionality to support user-defined operations and conducting comprehensive performance evaluation across multiple scenarios.</p>
<ul>
<li>
<strong>Custom Update Kernel Generation:</strong> Adapted the existing custom update framework for ISPC by applying <code>foreach</code> parallelization to user-defined mathematical operations and reduction algorithms</li>
<li>
<strong>Comprehensive Benchmarking Suite:</strong> Extensive performance tests were conducted for multiple neuron counts (4000, 8000, 16000, 32000, 64000) for all backends on both Windows native and WSL environments</li>
<li>
<strong>Performance Data Collection:</strong> Systematically gathered per-phase timing data and memory usage statistics to compare the performance achieved through SIMD vectorization with other backends</li>
</ul>
<p><strong>Benchmarking Methodology:</strong></p>
<ul>
<li>Utilized existing GeNN usage example code as the foundation for performance tests</li>
<li>Conducted comparative analysis against single-threaded CPU and cuda backends</li>
</ul>
<h4>Key Technical Contributions</h4>
<p><strong>SIMD Kernel Adaptation Strategy:</strong>
The core technical achievement involved the systematic refactoring of existing single-threaded algorithms into SIMD-optimized implementations. This was accomplished through strategic application of ISPC's <code>foreach</code> construct, which enabled automatic vectorization while preserving functional correctness.</p>
<p><strong>Backend Architecture Implementation:</strong></p>
<div class="code"><pre class="code literal-block"><span class="c1">// Core backend methods successfully implemented</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">genNeuronUpdate</span><span class="p">(</span><span class="n">CodeStream</span><span class="w"> </span><span class="o">&amp;</span><span class="n">os</span><span class="p">,</span><span class="w"> </span>
<span class="w">                     </span><span class="k">const</span><span class="w"> </span><span class="n">ModelSpec</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model</span><span class="p">,</span><span class="w"> </span>
<span class="w">                     </span><span class="k">const</span><span class="w"> </span><span class="n">NeuronGroupInternal</span><span class="w"> </span><span class="o">&amp;</span><span class="n">ng</span><span class="p">,</span><span class="w"> </span>
<span class="w">                     </span><span class="k">const</span><span class="w"> </span><span class="n">Substitutions</span><span class="w"> </span><span class="o">&amp;</span><span class="n">popSubs</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">genSynapseUpdate</span><span class="p">(</span><span class="n">CodeStream</span><span class="w"> </span><span class="o">&amp;</span><span class="n">os</span><span class="p">,</span>
<span class="w">                      </span><span class="k">const</span><span class="w"> </span><span class="n">ModelSpec</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model</span><span class="p">,</span>
<span class="w">                      </span><span class="k">const</span><span class="w"> </span><span class="n">SynapseGroupInternal</span><span class="w"> </span><span class="o">&amp;</span><span class="n">sg</span><span class="p">,</span>
<span class="w">                      </span><span class="k">const</span><span class="w"> </span><span class="n">Substitutions</span><span class="w"> </span><span class="o">&amp;</span><span class="n">popSubs</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">genCustomUpdate</span><span class="p">(</span><span class="n">CodeStream</span><span class="w"> </span><span class="o">&amp;</span><span class="n">os</span><span class="p">,</span><span class="w"> </span>
<span class="w">                     </span><span class="k">const</span><span class="w"> </span><span class="n">ModelSpec</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model</span><span class="p">,</span>
<span class="w">                     </span><span class="k">const</span><span class="w"> </span><span class="n">CustomUpdateInternal</span><span class="w"> </span><span class="o">&amp;</span><span class="n">cu</span><span class="p">,</span>
<span class="w">                     </span><span class="k">const</span><span class="w"> </span><span class="n">Substitutions</span><span class="w"> </span><span class="o">&amp;</span><span class="n">popSubs</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
</pre></div>

<p><strong>Vectorization Methodology:</strong></p>
<ul>
<li>
<strong>Foreach Parallelization:</strong> Systematically identified scalar loops in existing CPU backend and applied <code>foreach</code> constructs to enable SIMD processing</li>
<li>
<strong>Memory Layout Optimization:</strong> Implemented Array class to ensure optimal memory access patterns for vectorized operations</li>
<li>
<strong>Algorithm Preservation:</strong> Maintained exact functional behavior of original implementations while achieving significant performance improvements through parallelization</li>
</ul>
<p><strong>Integration Achievements:</strong></p>
<ul>
<li>Successful integration with GeNN's existing code generation pipeline</li>
<li>Full compatibility with PyGeNN Python interface</li>
<li>Capable of cross-platform deployment across Windows and Linux environments</li>
</ul>
<hr>
<h3>Results &amp; Analysis</h3>
<h4>Performance Benchmarking</h4>
<p><strong>Test Configuration:</strong></p>
<ul>
<li>Hardware: Intel Core i7-12700K (AVX2 support, 256bit-8 lanes)</li>
<li>Operating Systems: Windows 11, Ubuntu 22.04 (WSL2)</li>
<li>Comparison Backends:  ISPC, Single-thread CPU, CUDA (RTX 3050)</li>
</ul>
<p><strong>Benchmark Models: Vogels-Abbott Network</strong></p>
<ol>
<li><strong>Dense Network (4000, 8000, 16000 and 32000 neurons)</strong></li>
<li><strong>Sparse Network (4000, 8000, 16000, 32000 and 64000 neurons)</strong></li>
</ol>
<p><strong>Detailed Performance Data:</strong>
Complete benchmarking results, including raw timing data, memory usage statistics, and cross-platform comparisons are available in the <a href="https://docs.google.com/spreadsheets/d/1gJQmLC9h9WJT5Wl508GS2M-N4OALJ9hQ410A1KKgGMU/edit?usp=sharing">Performance Analysis Spreadsheet</a>.</p>
<h4>Performance Results</h4>
<p><strong>Comprehensive Benchmarking Across Multiple Scales:</strong></p>
<p><strong>Sparse Networks:</strong></p>
<ul>
<li>Single-thread CPU: 1.0x baseline</li>
<li>ISPC on i7 12700H (AVX2): 1.4x speedup</li>
</ul>
<p><strong>Dense Networks:</strong></p>
<ul>
<li>Single-thread CPU: 1.0x baseline</li>
<li>ISPC on Intel i5 (AVX2): 3.05x speedup </li>
<li>ISPC on i7 12700H (AVX2): 3.1x speedup</li>
<li>ISPC on Xeon Gold 6134 (AVX512): 9.49x speedup </li>
</ul>
<p><strong>Cross-Platform Performance Comparison:</strong></p>
<ul>
<li>
<strong>Windows vs WSL2 Single-threaded:</strong> WSL2 demonstrated inferior single-threaded performance and superior ISPC performance</li>
<li>
<strong>ISPC Performance:</strong> WSL2 ISPC implementation achieved 50-60% execution time of Windows ISPC for dense networks</li>
<li>
<strong>Sparse Network Optimization:</strong> WSL2 ISPC sparse networks executed in approximately 0.35x the time of Windows implementation</li>
<li>
<strong>Memory Bandwidth Utilization:</strong> Achieved 60-75% of theoretical peak across all test configurations. This was noted through the CPU utilizataion graph</li>
</ul>
<h4>Key Observations</h4>
<ol>
<li>
<strong>Unexpected WSL2 Performance Advantage</strong>: Contrary to expectations, WSL2 demonstrated superior performance for ISPC implementations, with ISPC for dense tests achieving 40-50% better execution times than Windows native</li>
<li>
<strong>Hardware-Dependent Scaling</strong>: Significant performance variation was observed across different CPU architectures, with Xeon Gold 6134 achieving 9.49x speedup compared to 3.05x on Intel i5. This is due to the advanced ISA on the Xeon Gold 6134 allowing 16 parallel lanes</li>
<li>
<strong>SIMD Vectorization Efficiency</strong>: Achieved 60% of theoretical SIMD peak performance across all tested configurations on AVX-512 and ~40% on AVX-256 ISA </li>
<li>
<strong>Memory Subsystem Optimization</strong>: WSL2's memory management appears better optimized for SIMD workloads, particularly benefiting sparse network computations (~0.35x of Windows ISPC simulation time)</li>
<li>
<strong>Cross-Platform Portability</strong>: Successful deployment across Windows and Linux environments with platform-specific performance characteristics</li>
<li>
<strong>Vectorization Success</strong>: Successful adaptation of existing scalar algorithms to SIMD paradigm without algorithmic modifications, maintaining numerical accuracy across platforms</li>
</ol>
<hr>
<h3>Challenges Faced</h3>
<h4>Technical Challenges</h4>
<p><strong>1. Understanding GeNN's Complex Architecture:</strong>
GeNN is a well-structured library with intricate code generation pipelines and backend methods. Before any implementation could begin, I invested time in studying the existing backends and their design patterns. With guidance from my mentor, I developed a clear understanding of how different components interact, which formed the foundation for all subsequent development work.</p>
<p><strong>2. Build System Integration:</strong>
Integrating ISPC compiler and build dependencies into GeNN's existing CMake-based build system was tricky. My mentor's assistance in configuring library linking and cross-platform compilation was particularly helpful in building the ISPC backend.</p>
<p><strong>3. Dual Code Generation Strategy:</strong>
ISPC keywords are not recognised in a standard C++ file and therefore the backend required managing two separate code streams - C++ host code (for .cc files) and ISPC kernel code (for .ispc files) with their respective dependencies. Initialization was managed in the C++ file while parallel computations were managed in the ISPC ones. This helped in achieving a clean code organization and optimal performance.</p>
<hr>
<h3>Future Improvements</h3>
<p><strong>1. Batch Size Optimization:</strong></p>
<ul>
<li>Implement support for batch sizes greater than 1 to process multiple simulation steps simultaneously</li>
<li>Leverage SIMD width more effectively by processing multiple timesteps in parallel</li>
<li>Optimize memory access patterns for batched operations to improve cache utilization</li>
</ul>
<p><strong>2. Automatic Instruction Set Detection:</strong></p>
<ul>
<li>Implement runtime detection of optimal SIMD instruction set architecture (SSE, AVX, AVX2, AVX-512)</li>
<li>Automatically select the best performing instruction set based on available hardware capabilities</li>
<li>Provide fallback mechanisms for older processors while maximizing performance on newer architectures</li>
</ul>
<p><strong>3. Native ISPC Implementation of Core Functions:</strong></p>
<ul>
<li>Implement Random Number Generation (RNG) and other utility methods directly in ISPC</li>
<li>Reduce time spent on C++ initialization by moving more functionality to ISPC kernels</li>
</ul>
<hr>
<h3>Conclusion</h3>
<p>The development of an ISPC backend for GeNN successfully addresses the performance gap between single-threaded CPU and GPU implementations. The project achieved its primary objectives by delivering a fully functional backend that provides significant performance improvements while maintaining compatibility with existing GeNN models.</p>
<h4>Key Achievements</h4>
<ol>
<li>
<strong>Performance Impact</strong>: Delivered significant speedup over single-threaded CPU implementations</li>
<li>
<strong>Accessibility</strong>: Enabled high-performance neural simulations on standard CPU hardware</li>
<li>
<strong>Portability</strong>: Provided cross-platform compatibility across Windows, Linux, and macOS</li>
<li>
<strong>Integration</strong>: Seamlessly integrated with existing GeNN ecosystem and PyGeNN interface</li>
</ol>
<h4>Community Impact</h4>
<p>The ISPC backend significantly lowers the barrier to entry for high-performance neural network simulations. Researchers without access to specialized GPU hardware can now achieve considerable performance jumps for medium-scale simulations. This democratization of computational neuroscience tools aligns with GeNN's mission to make neural network simulation accessible to a broader research community.</p>
<p>The successful completion of this project establishes a foundation for future developments in CPU-based neural network acceleration and demonstrates the viability of SIMD programming for computational neuroscience applications.</p>
<hr>
<h3>Acknowledgments</h3>
<p>I would like to express my sincere gratitude to my mentors, <strong>Dr. Jamie Knight</strong> and <strong>Dr. Thomas Nowotny</strong>, whose invaluable guidance, expertise, and continuous support made this project possible. Their deep knowledge of GeNN's architecture and SIMD programming principles was instrumental in navigating the complexities of backend development and achieving the project's objectives.</p>
<p>Special thanks to Dr. Knight for his assistance with the build system integration and initialization architecture. His mentorship not only helped me complete this project successfully but also significantly aided my understanding of high-performance computing and computational neuroscience.</p>
<p>I am also grateful to the INCF organization and the GeNN development team for providing this opportunity through Google Summer of Code 2025, and for their commitment to advancing open-source tools in computational neuroscience.</p>
<hr>
<h3>References</h3>
<ol>
<li>
<p>Intel Corporation. (2023). <em>Intel SPMD Program Compiler User's Guide</em>. <a href="https://ispc.github.io/">Available online</a></p>
</li>
<li>
<p>Intel Corporation. (2013). <em>SIMD Made Easy with Intel ISPC</em>. <a href="https://www.intel.com/content/dam/develop/external/us/en/documents/simd-made-easy-with-intel-ispc.pdf">Available online</a></p>
</li>
<li>
<p>Pharr, M., &amp; Mark, W. R. (2012). ispc: A SPMD compiler for high-performance CPU programming. <em>Proceedings of Innovative Parallel Computing (InPar)</em>. <a href="https://ieeexplore.ieee.org/document/6339601">Available online</a></p>
</li>
<li>
<p>Yavuz, E., Turner, J., &amp; Nowotny, T. (2016). GeNN: a code generation framework for accelerated brain simulations. <em>Scientific Reports</em>, 6, 18854. <a href="https://www.nature.com/articles/srep18854">Available online</a></p>
</li>
<li>
<p>Knight, J. C., Komissarov, A., &amp; Nowotny, T. (2021). PyGeNN: A Python Library for GPU-Enhanced Neural Networks. <em>Frontiers in Neuroinformatics</em>, 15, 659005. <a href="https://www.frontiersin.org/journals/neuroinformatics/articles/10.3389/fninf.2021.659005/full">Available online</a></p>
</li>
<li>
<p>Vogels, T. P., &amp; Abbott, L. F. (2005). Signal propagation and logic gating in networks of integrate-and-fire neurons. <em>Journal of Neuroscience</em>, 25(46), 10786-10795. <a href="https://www.jneurosci.org/content/25/46/10786">Available online</a></p>
</li>
<li>
<p>Hennessy, J. L., &amp; Patterson, D. A. (2019). <em>Computer architecture: a quantitative approach</em>. Morgan Kaufmann.</p>
</li>
</ol>
<hr>
<p><em>This report was prepared as part of the Google Summer of Code 2025 program.</em></p>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="../posts/running-away.html" class="u-url">Software Developer Blog: Running away</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2022-11-01T13:35:07Z" title="2022-11-01 13:35">2022-11-01 13:35</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>After spending a long time updating GeNN's code generator to generate more efficient CUDA kernels which have the side benefit of compiling much more quickly, there remained something of a dirty secret.
The <code>runner.cc</code> file which contains the helper functions generated by GeNN for allocating memory and copying variables between GPU and CPU could still easily grow to the point that compilation would take an extremely long time and consume all available memory.
For our <a href="https://github.com/neworderofjamie/multi-area-model/">multi-area model implementation</a>, I added various options which turn off the generation of empty functions and, as everything in this model was generated on the GPU anyway, I also turned off the generation of host copies of almost all variables.
This resulted in a paltry 40 mbyte <code>runner.cc</code> which compiled in a couple of minutes which, for a model this size, is just about acceptable.
However, as users have started making bigger models and not always wanting to generate everything on the GPU, this issue has kept reappearing.</p>
<h2>Jinjaly investigating</h2>
<p>To investigate this in a slightly simpler way than just building larger and larger GeNN models until things break, I used <a href="https://jinja.palletsprojects.com">Jinja</a> to build a template that could generate fake <code>runner.cc</code> files containing varying number of arrays, representing the state variables in a real model. 
The heart of this template looked something like this:</p>
<div class="code"><pre class="code literal-block"><span class="c1">// Push and pull functions</span>
<span class="p">{</span><span class="o">%</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">array</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">arrays</span><span class="w"> </span><span class="o">%</span><span class="p">}</span>
<span class="kt">void</span><span class="w"> </span><span class="n">push</span><span class="p">{{</span><span class="n">array</span><span class="p">.</span><span class="n">name</span><span class="p">}}</span><span class="n">ToDevice</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">CHECK_CUDA_ERRORS</span><span class="p">(</span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_</span><span class="p">{{</span><span class="n">array</span><span class="p">.</span><span class="n">name</span><span class="p">}},</span><span class="w"> </span><span class="p">{{</span><span class="n">array</span><span class="p">.</span><span class="n">name</span><span class="p">}},</span><span class="w"> </span>
<span class="w">                                 </span><span class="p">{{</span><span class="n">array</span><span class="p">.</span><span class="n">size</span><span class="p">}}</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span>
<span class="w">                                 </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">));</span>
<span class="p">}</span>

<span class="kt">void</span><span class="w"> </span><span class="n">pull</span><span class="p">{{</span><span class="n">array</span><span class="p">.</span><span class="n">name</span><span class="p">}}</span><span class="n">FromDevice</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">CHECK_CUDA_ERRORS</span><span class="p">(</span><span class="n">cudaMemcpy</span><span class="p">({{</span><span class="n">array</span><span class="p">.</span><span class="n">name</span><span class="p">}},</span><span class="w"> </span><span class="n">d_</span><span class="p">{{</span><span class="n">array</span><span class="p">.</span><span class="n">name</span><span class="p">}},</span><span class="w"> </span>
<span class="w">                                 </span><span class="p">{{</span><span class="n">array</span><span class="p">.</span><span class="n">size</span><span class="p">}}</span><span class="w">  </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span>
<span class="w">                                 </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">));</span>
<span class="p">}</span>
<span class="p">{</span><span class="o">%</span><span class="w"> </span><span class="n">endfor</span><span class="w"> </span><span class="o">%</span><span class="p">}</span>

<span class="kt">void</span><span class="w"> </span><span class="n">allocateMem</span><span class="p">()</span><span class="w"> </span>
<span class="p">{</span>
<span class="w">    </span><span class="n">CHECK_CUDA_ERRORS</span><span class="p">(</span><span class="n">cudaSetDevice</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>

<span class="w">    </span><span class="p">{</span><span class="o">%</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">array</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">arrays</span><span class="w"> </span><span class="o">%</span><span class="p">}</span>
<span class="w">    </span><span class="n">CHECK_CUDA_ERRORS</span><span class="p">(</span><span class="n">cudaHostAlloc</span><span class="p">(</span><span class="o">&amp;</span><span class="p">{{</span><span class="n">array</span><span class="p">.</span><span class="n">name</span><span class="p">}},</span><span class="w"> </span>
<span class="w">                                    </span><span class="p">{{</span><span class="n">array</span><span class="p">.</span><span class="n">size</span><span class="p">}}</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span>
<span class="w">                                    </span><span class="n">cudaHostAllocPortable</span><span class="p">));</span>
<span class="w">    </span><span class="n">CHECK_CUDA_ERRORS</span><span class="p">(</span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_</span><span class="p">{{</span><span class="n">array</span><span class="p">.</span><span class="n">name</span><span class="p">}},</span><span class="w"> </span>
<span class="w">                                 </span><span class="p">{{</span><span class="n">array</span><span class="p">.</span><span class="n">size</span><span class="p">}}</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)));</span>
<span class="w">    </span><span class="p">{</span><span class="o">%</span><span class="w"> </span><span class="n">endfor</span><span class="w"> </span><span class="o">%</span><span class="p">}</span><span class="w">    </span>
<span class="p">}</span>
</pre></div>

<p>this template (saved in <code>runner.cc.template</code>) could then be used to generate C++ and print it to stdout like:</p>
<div class="code"><pre class="code literal-block"><span class="kn">from</span> <span class="nn">jinja2</span> <span class="kn">import</span> <span class="n">Template</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"runner.cc.template"</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">template</span> <span class="o">=</span> <span class="n">Template</span><span class="p">(</span><span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>

<span class="n">arrays</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">"name"</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"array_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span> <span class="s2">"size"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">}</span> 
          <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_arrays</span><span class="p">)]</span>


<span class="nb">print</span><span class="p">(</span><span class="n">template</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">arrays</span><span class="o">=</span><span class="n">arrays</span><span class="p">))</span>
</pre></div>

<p>On Linux, the C++ could then be built using the same command line used by GeNN itself (some options omitted for brevity) and timed using the <code>/usr/bin/time</code> (I recently discovered that <code>command time</code> can be used as an alternative way of disambiguating this from the bash builtin <code>time</code>):</p>
<div class="code"><pre class="code literal-block">/usr/bin/time<span class="w"> </span>-v<span class="w"> </span>nvcc<span class="w"> </span>-c<span class="w"> </span>-x<span class="w"> </span>cu<span class="w"> </span>-arch<span class="w"> </span>sm_86<span class="w"> </span>-std<span class="o">=</span>c++11<span class="w"> </span>test.cc
</pre></div>

<p>Initial experiments showed that while both the wall clock time and maximum resident set size (roughly analagous to peak physical memory usage) grow approximately linearly (much to my relief after reading way too many <a href="https://randomascii.wordpress.com/category/quadratic/">horror stories</a> on Bruce Dawson's <a href="https://randomascii.wordpress.com">excellent blog</a>) with the number of arrays, it still grew extremely rapidly:</p>
<p><img alt="Relationship between runner size; and compile time and memory usage" src="../images/blog_running_away/fig_1_linearity.png"></p>
<p>Therefore, a model with 10000 arrays will take over 4 minutes and around 8 gbyte of memory to compile — neither of which are really acceptable.
To put this in perspective, if you split a model up into about 100 populations and connect most of the permutations together (this is an all-too-reasonable assumption in many areas of the mammalian brain), you could easily reach this many variables.</p>
<p>So....what is NVCC doing with all this time and memory?
<code>runner.cc</code> only contains host code (NVCC is just used to ensure the same compiler/options across execution units and to deal with setting up the CUDA linker/include paths) but, when you pass a 5 mbyte <code>runner.cc</code> file to NVCC, the file that is passed on to the host compiler (GCC) has grown to 15 mbyte!
However, this turned out to be simply because NVCC is in charge of running the preprocessor so that 10 mbyte is 'just' the result of expanding macros and including C++ standard library header files!</p>
<h2>Profiling</h2>
<p>Around this point, I remembered reading a blog post about <a href="https://aras-p.info/blog/2019/01/12/Investigating-compile-times-and-Clang-ftime-report/">profiling compiler times</a> on yet another <a href="https://aras-p.info/blog/">excellent blog</a> and turned on the <code>-ftime-report</code> GCC option.
As the blog promised, this generates a gargantuan report which starts by splitting the compilation time of this 10000 array model into 'phases':</p>
<div class="code"><pre class="code literal-block">phase setup             :  ...   0.00 ( 0%) wall    1384 kB ( 0%)
phase parsing           :  ...  20.81 ( 9%) wall 1794944 kB (25%)
phase lang. deferred    :  ...   0.02 ( 0%) wall    2426 kB ( 0%)
phase opt and generate  :  ... 214.14 (91%) wall 5412439 kB (75%)
phase finalize          :  ...   0.54 ( 0%) wall       0 kB ( 0%)
</pre></div>

<p>Somewhat surprisingly (as, after all, we're throwing a massive source file at GCC), the vast majority of time is spent in "opt and generate" (code-generation and optimisation) rather than in parsing.
Looking a little further down, where the report contains a seeminly unsorted list of processes within phases, the only other 'hot' line is:</p>
<div class="code"><pre class="code literal-block">expand vars             :  ... 101.93 (43%) wall   50597 kB ( 1%)
</pre></div>

<p>but, as a non-GCC developer, this doesn't help me a great deal....back to pursuing random hunches!</p>
<h2>Smoking gun</h2>
<p>As there's no virtual functions in this code, my C++ prejudices suggest that only exceptions could <em>possibly</em> be to blame and, as each of those <code>CHECK_CUDA_ERRORS</code> macros hides a <code>throw std::runtime_error</code>, maybe that's not unreasonable.
Generating all that zero-cost abstraction must involve <em>expanding</em> a lot of <em>variables</em>....right!?
How about if we replace our current implemementation of <code>CHECK_CUDA_ERRORS</code>:</p>
<div class="code"><pre class="code literal-block"><span class="cp">#define CHECK_CUDA_ERRORS(call) {\</span>
<span class="cp">    cudaError_t error = call;\</span>
<span class="cp">    if(error != cudaSuccess) {\</span>
<span class="cp">        throw std::runtime_error(__FILE__": " \</span>
<span class="cp">                                 + std::to_string(__LINE__) \</span>
<span class="cp">                                 + ": cuda error " \</span>
<span class="cp">                                 + std::to_string(error) \</span>
<span class="cp">                                 + ": " + cudaGetErrorString(error));\</span>
<span class="cp">    }\</span>
<span class="cp">}</span>
</pre></div>

<p>with:</p>
<div class="code"><pre class="code literal-block"><span class="cp">#define CHECK_CUDA_ERRORS(call) {\</span>
<span class="cp">    cudaError_t error = call;\</span>
<span class="cp">    assert(error == cudaSuccess); \</span>
<span class="cp">}</span>
</pre></div>

<p>or even:</p>
<div class="code"><pre class="code literal-block"><span class="cp">#define CHECK_CUDA_ERRORS(call) {\</span>
<span class="cp">    cudaError_t error = call;\</span>
<span class="cp">    if(error != cudaSuccess) {\</span>
<span class="cp">        std::abort();\</span>
<span class="cp">    }\</span>
<span class="cp">}</span>
</pre></div>

<p>Some template-meddling and sweeping later we can produce:</p>
<p><img alt="Relationship between number of arrays and error handling approach; and compile time and memory usage" src="../images/blog_running_away/fig_2_alternate_error.png"></p>
<p>Seems like this actually works! Our 10000 array model now only takes 30 seconds and less than 1 gbyte of memory to compile which is much more reasonable!</p>
<p>However, are the exceptions <em>really</em> to blame? 
Compile times seem much better when using the single-threaded CPU backend and that allocates memory with <code>new[]</code> (obviously, modern C++ rules don't apply in generated code...) which throws <code>std::bad_alloc</code> to signal failure.
Admittedly, because there's no need to copy data when everthing's on the CPU, this backend generates empty 'push' and 'pull' functions so there's less code to compile overall but, if generating exception handling code was the problem, you would expect issues here too.
Maybe expanding all that message-generating code is the real issue...
How about we hack the following additional variants into the template:</p>
<div class="code"><pre class="code literal-block"><span class="cp">#define CHECK_CUDA_ERRORS(call) {\</span>
<span class="cp">    cudaError_t error = call;\</span>
<span class="cp">    if(error != cudaSuccess) {\</span>
<span class="cp">        throw std::runtime_error();\</span>
<span class="cp">    }\</span>
<span class="cp">}</span>
</pre></div>

<p>and</p>
<div class="code"><pre class="code literal-block"><span class="cp">#define CHECK_CUDA_ERRORS(call) {\</span>
<span class="cp">    cudaError_t error = call;\</span>
<span class="cp">    if(error != cudaSuccess) {\</span>
<span class="cp">        std::cerr &lt;&lt; __FILE__ &lt;&lt; ": " &lt;&lt; __LINE__;\</span>
<span class="cp">        std::cerr &lt;&lt; ": cuda error " &lt;&lt; error &lt;&lt; ": ";\</span>
<span class="cp">        std::cerr &lt;&lt; cudaGetErrorString(error) &lt;&lt; std::endl;\</span>
<span class="cp">        std::abort();\</span>
<span class="cp">    }\</span>
<span class="cp">}</span>
</pre></div>

<p>and sweep:</p>
<p><img alt="Relationship between number of arrays and error handling approach; and compile time and memory usage" src="../images/blog_running_away/fig_3_more_alternate_error.png"></p>
<p><code>std::abort</code> is definitely easier on the compiler than throwing exceptions but, compiling the message generation code also seems to make a large difference.</p>
<h2>MSVC</h2>
<p>The other compiler I often use with GeNN is Microsoft Visual C++.
I can't quite face repeating this whole process again but, initial tests suggest that this optimisation is even more valuable here.
Using the simplest <code>std::abort</code> raising <code>CHECK_CUDA_ERRORS</code> macro, the 10000 array model can be compiled in around 19 seconds whereas, using the original exception-throwing macro...I have given up waiting after around 1 hour!</p>
<h2>Implementing a workaround</h2>
<p>GeNN is approaching the end of the 4.X release cycle so, for now, I have added a simple but slightly hacky workaround for these issues by adding a <code>generateSimpleErrorHandling</code> flag to GeNN's CUDA backend to switch from generating code with the previous full-fat <code>CHECK_CUDA_ERRORS</code> macro to the simplest version which simply calls <code>std::abort</code> without generating a message. This can be turned on from C++ like:</p>
<div class="code"><pre class="code literal-block"><span class="kt">void</span><span class="w"> </span><span class="nf">modelDefinition</span><span class="p">(</span><span class="n">NNmodel</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="w">    </span><span class="n">GENN_PREFERENCES</span><span class="p">.</span><span class="n">generateSimpleErrorHandling</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">}</span>
</pre></div>

<p>or from Python like:</p>
<div class="code"><pre class="code literal-block"><span class="n">model</span> <span class="o">=</span> <span class="n">GeNNModel</span><span class="p">(</span><span class="s2">"float"</span><span class="p">,</span> <span class="s2">"my_model"</span><span class="p">,</span> <span class="n">generateSimpleErrorHandling</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

<h2>Real models</h2>
<p>The largest model we currently have to play with with is the <a href="https://github.com/neworderofjamie/multi-area-model/">multi-area cortical model</a>.
Although it has 64516 synapse groups, due to its use of procedural connectivity (where all synaptic connectivity, weights and delays are generated on the fly), it doesn't actually have any per-synapse group variables with push and pull functions.
Nonetheless, using the new simple error handling reduces the compilation time of the <code>runner.cc</code> from 155 to 129 seconds.
Finally, although it is not possible to <em>run</em> the model in this way as no single GPU has enough memory, we can generate a <code>runner.cc</code> from this model with standard, in-memory sparse connectivity and push and pull functions for each variable.
This results in a nightmarish, 114 mbyte <code>runner.cc</code> which, using the original <code>CHECK_CUDA_ERRORS</code> macro, would definitely be impossible to compile on any reasonable machine.
However, using the new simplified macro, the runner can be compiled in just over 20 minutes and requires just over 14 gbyte of memory — still pretty unusable but definitely progress!</p>
<h2>Long-term solutions</h2>
<p>The majority of the time, the errors which the <code>CHECK_CUDA_ERRORS</code> macro is aiming to catch are out of memory errors in the <code>allocateMem</code> function and errors that occured during (asynchronous) kernel launches that are only caught at the next push or pull call (which are typically the main synchronisation points) so perhaps, in future, we could adopt a more targetted error-handling approach which provides a balance between sufficient debugging information and compilation time.</p>
<p>However, while the error handling changes discussed here allow the current approach to generating <code>runner.cc</code> files to scale a bit further, the code we are generating is still pretty pathological, least of all because <a href="https://github.com/genn-team/genn/issues/408">the Windows PE executable format has a limit of 65535 symbol limit</a> which you can hit quite easily with a large model.
Early this year, I made an attempt at re-writing the code generator to apply the same merging strategy GeNN uses elsewhere to <code>runner.cc</code>. 
This means that all the variables associated with neuron and synapse populations with the same types of state variable can be allocated using one piece of shared generated code.
While this works, it adds yet more complexity to GeNN and fundamentally breaks the 'classic' way of using GeNN from C++, where you link some C++ simulation code against your generated code and can access state variables directly by name.
However, based on this investigation, maybe that project needs resurrecting!</p>
<p>All the code I've developed to explore this problem is available from <a href="https://github.com/neworderofjamie/nvcc_breaker">my Github</a>.</p>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="../posts/sw_blog_toeplitz.html" class="u-url">Software Developer Blog: How to do convolutions with doubly blocked Toeplitz matrices</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2021-12-21T14:39:44Z" title="2021-12-21 14:39">2021-12-21 14:39</time></span>
        </div>
    </header><div class="e-content entry-content">
    <h2>How to do convolutions with doubly blocked Toeplitz matrices</h2>
<p>A few weeks ago, Jamie (@neworderofjamie) asked me on the chat whether I knew what doubly blocked Toeplitz matrices are and how they implement convolutions. I had no clue. Since then we have implemented convolutions using doubly blocked Toeplitz matrices in GeNN and found them to be extremely useful and efficient.
1
In this software blog I will give a brief overview on the why and how convolutions relate to doubly blocked Toeplitz matrices. My blog is based on Ali Salehi's tutorial <a href="https://raw.githubusercontent.com/alisaaalehi/convolution_as_multiplication/master/ConvAsMulExplained.pdf">Convolution as Matrix Multiplication</a>  but updated to use machine-learning rather than signal-processing conventions and I am trying to avoid using too many unusual ways of re-arranging rows and columns.</p>
<h3>The why</h3>
<p>Let us consider the convolution of a \(2\times 2\) kernel with a \(3\times 3\) layer. We denote the kernel as
\[
K= \left(\matrix{
k_{11} &amp; k_{12} \cr
k_{21} &amp; k_{22}}\right)
\]
and the layer as
\[
I= \left(\matrix{
i_{11} &amp; i_{12} &amp; i_{13} \cr
i_{21} &amp; i_{22} &amp; i_{23} \cr
i_{31} &amp; i_{32} &amp; i_{33}
} \right).
\]
Then the convolution in the machine learning use of the term is calculating the cross-correlation of the kernel "moving across" the layer as illustrated below. The layer \(I\) is in blue, the kernel \(K\) in grey and the result \(R\) in green.</p>
<table>
<thead><tr>
<th style="text-align: center;">
<sup id="fnref:1"><a class="footnote-ref" href="../posts/sw_blog_toeplitz.html#fn:1">1</a></sup><img alt="Illustration of convolution step" src="../images/blog_00.png">
</th>
<th style="text-align: center;"><img alt="Illustration of convolution step" src="../images/blog_01.png"></th>
<th style="text-align: center;"><img alt="Illustration of convolution step" src="../images/blog_02.png"></th>
<th style="text-align: center;"><img alt="Illustration of convolution step" src="../images/blog_03.png"></th>
</tr></thead>
<tbody><tr>
<td style="text-align: center;">\(r_{11}\)</td>
<td style="text-align: center;">\(r_{12}\)</td>
<td style="text-align: center;">\(r_{13}\)</td>
<td style="text-align: center;">\(3_{14}\)</td>
</tr></tbody>
</table>
<p>For the first non-zero entry at \((1,1)\) of the result matrix \(R\), we therefore have \(r_{11} = k_{22} i_{11}\).
Then the kernel moves one over and \(r_{12} = k_{21}i_{11} + k_{22} i_{12}\). Then, \(r_{13} = k_{21}i_{12} + k_{22} i_{13}\) and \(r_{14} = k_{21}i_{13} \).</p>
<table>
<thead><tr>
<th style="text-align: center;"><img alt="Illustration of convolution step" src="../images/blog_04.png"></th>
<th style="text-align: center;"><img alt="Illustration of convolution step" src="../images/blog_05.png"></th>
<th style="text-align: center;"><img alt="Illustration of convolution step" src="../images/blog_06.png"></th>
<th style="text-align: center;"><img alt="Illustration of convolution step" src="../images/blog_07.png"></th>
</tr></thead>
<tbody><tr>
<td style="text-align: center;">\(r_{21}\)</td>
<td style="text-align: center;">\(r_{22}\)</td>
<td style="text-align: center;">\(r_{23}\)</td>
<td style="text-align: center;">\(r_{24}\)</td>
</tr></tbody>
</table>
<p>So, for the second row,
\(r_{21} = k_{12} i_{11} + k_{22} i_{21} \), move one over,
\(r_{22} = k_{11} i_{11} + k_{12} i_{12} + k_{21} i_{21} + k_{22} i_{22} \), one more to the right,
\(r_{23} = k_{11}i_{12} + k_{12} i_{13} + k_{21} i_{22} + k_{22} i_{23} \), and finally
\(r_{24} = k_{11}i_{13} + k_{21} i_{23} \).</p>
<p>It works similar for the remaining two rows.</p>
<p>If we unroll the layer \(I\) row-wise into a column vector \(I_\text{col}\),
\[
    I_\text{col} = 
    \left(
    \matrix{ 
        i_{11} \cr
        i_{12} \cr
        i_{13} \cr
        i_{21} \cr
        i_{22} \cr
        i_{23} \cr
        i_{31} \cr
        i_{32} \cr
        i_{33}
        }
    \right),
\]
then we can express this as a matrix-vector multiplication of a matrix formed from the entries of the kernel \(K\) and the vector\(I_\text{col}\), 
\[
\left(\matrix{
k_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cr
k_{21} &amp; k_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cr
0 &amp; k_{21} &amp; k_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cr
0 &amp; 0 &amp; k_{21} &amp; k_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cr
k_{12} &amp; 0 &amp; 0 &amp; k_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cr
k_{11} &amp; k_{12} &amp; 0 &amp; k_{21} &amp; k_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cr
0 &amp; k_{11} &amp; k_{12} &amp; 0 &amp; k_{21} &amp; k_{22} &amp; 0 &amp; 0 &amp; 0 \cr
0 &amp; 0 &amp; k_{11} &amp; 0 &amp; 0 &amp; k_{21} &amp; 0 &amp; 0 &amp; 0 \cr
0 &amp; 0 &amp; 0 &amp; k_{12} &amp; 0 &amp; 0 &amp; k_{22} &amp; 0 &amp; 0 \cr
0 &amp; 0 &amp; 0 &amp; k_{11} &amp; k_{12} &amp; 0 &amp; k_{21} &amp; k_{22} &amp; 0 \cr
0 &amp; 0 &amp; 0 &amp; 0 &amp; k_{11} &amp; k_{12} &amp; 0 &amp; k_{21} &amp; k_{22} \cr
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; k_{11} &amp; 0 &amp; 0 &amp; k_{21} \cr
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; k_{12} &amp; 0 &amp; 0 \cr
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; k_{11} &amp; k_{12} &amp; 0 \cr
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; k_{11} &amp; k_{12} \cr
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; k_{11} }\right)
\cdot
\left(\matrix{
i_{11} \cr
i_{12} \cr
i_{13} \cr
i_{21} \cr
i_{22} \cr
i_{23} \cr
i_{31} \cr
i_{32} \cr
i_{33}} 
\right)
\]</p>
<p>Now one can already see that the matrix formed from the kernel entries has a very peculiar shape - the shape of a doubly blocked Toeplitz matrix</p>
<h3>Doubly blocked Toeplitz matrix</h3>
<p>A Toeplitz matrix is a matrix where the values along all diagonals are constant, i.e.</p>
<p>\[
\left(
    \matrix{ 
        a_{0} &amp; a_{-1} &amp; a_{-2} &amp; \cdots  &amp; \cdots &amp; \cdots &amp; a_{-(N-1)} \cr
        a_{1} &amp; a_{0} &amp; a_{-1} &amp; a_{-2} &amp;  &amp; &amp; \vdots \cr
        a_{2} &amp; a_{1} &amp; a_{0} &amp; a_{-1} &amp;  &amp; &amp; \vdots \cr
        \vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; &amp; \vdots \cr
        \vdots &amp; &amp; &amp; \ddots  &amp; a_{0} &amp; a_{-1} &amp; a_{-2} \cr
        \vdots &amp; &amp; &amp;  &amp; a_{1} &amp; a_{0} &amp; a_{-1} \cr
        a_{M-1} &amp; \cdots  &amp; \cdots &amp; \cdots &amp; a_{2} &amp; a_{1} &amp; a_{0} }
    \right) .
\]</p>
<p>Furthermore, if we build a matrix \(A\) out of Toeplitz sub-matrices \(A_{k}\) <em>and</em> the structure of \(A\) with respect to these submatrices is also Toeplitz:</p>
<p>\[
    A = \left(
    \matrix{ 
        A_{0} &amp; A_{-1} &amp; \cdots &amp; A_{-(L-1)} \cr
        A_{1} &amp; A_{0} &amp; \cdots &amp; A_{-(L-2)} \cr
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \cr
        A_{K} &amp; A_{K-1} &amp; \cdots &amp; A_{0}}
    \right),
\]</p>
<p>then, this matrix is called a doubly-blocked Toeplitz matrix. A standard way to generate a Toeplitz matrix from a vector \(v\) is to use \(v\) as the first column vector, then make one cyclic permutation and use it as the second column vector and so on.</p>
<h3>The method</h3>
<p>As we have seen on the example above, 2D convolution operations can be expressed as multiplication by a doubly-blocked Toeplitz matrix. As a general method, applied to the example above,
to convolve \(K\) with \(I\), we first flip \(K\) across the horizontal and vertical axis and pad it to the output size \((I_\text{height} + K_\text{height} - 1) \times (I_\text{width} + K_\text{width} - 1)\) of the convolution.
For instance, here, the \(3 \times 3\) layer \(I\) covolved by \(K\) above, leads to output size \(4 \times 4\).
Depending on the padding mode used by the convolution, typically, only part of this output is actually required.
The flipped and padded kernel \(K\) from above is
\[
    K_\text{pad}=
    \left(
    \matrix{ 
        k_{22} &amp; k_{21} &amp; 0 &amp; 0 \cr
        k_{12} &amp; k_{11} &amp; 0 &amp; 0 \cr
        0 &amp; 0 &amp; 0 &amp; 0 \cr
        0 &amp; 0 &amp; 0 &amp; 0 }
    \right)
\]</p>
<p>We then convert each <em>row vector</em> of this matrix into Toeplitz matrices \(F_i\) as described above:
\[
    F_0=
    \left(
    \matrix{ 
        k_{22} &amp; 0 &amp; 0 \cr
        k_{21} &amp; k_{22} &amp; 0 \cr
        0 &amp; k_{21} &amp; k_{22} \cr
        0 &amp; 0 &amp; k_{21}}
    \right)
    \quad
    F_1=
    \left(
    \matrix{ 
        k_{12} &amp; 0 &amp;  0 \cr
        k_{11} &amp; k_{12} &amp; 0 \cr
        0 &amp;  k_{11} &amp; k_{12} \cr
        0 &amp;  0 &amp;  k_{11}}
    \right)
    \]
    \[
    F_2=
    \left(
    \matrix{ 
        0 &amp; 0  &amp; 0 \cr
        0 &amp; 0 &amp; 0 \cr
        0  &amp; 0 &amp; 0 \cr
        0  &amp; 0  &amp; 0}
    \right)
    \quad
    F_3=
    \left(
    \matrix{ 
        0 &amp; 0  &amp; 0 \cr
        0 &amp; 0 &amp; 0 \cr
        0  &amp; 0 &amp; 0 \cr
        0  &amp; 0  &amp; 0}
    \right)
\]
and, finally, assemble these into a doubly blocked Toeplitz matrix \(F\):</p>
<p>\[
    F=
    \left(
    \matrix{ 
        F_0 &amp; F_3 &amp; F_2 \cr
        F_1 &amp; F_0 &amp; F_3 \cr
        F_2 &amp; F_1 &amp; F_0 \cr
        F_3 &amp; F_2 &amp; F_1
    }
    \right)
\]</p>
<p>The convolution of \(K\) with \(I\)
is then given by multiplying F from the left onto \(I_\text{col}\) as defined above,
\[
  R_{\text{col}} = F \cdot I  \quad 
  \Leftrightarrow \quad R_{\text{col},j}= \sum_i F_{ji}I_i 
  \]</p>
<p>Finally, \(R_{\text{col}}\) can be reinterpreted as the output matrix \(R\) by arranging its entries row-wise in a \(4\times 4\) matrix.</p>
<p>There we have it - convolution (in the machine learning sense, i.e. corss-correlation) of a kernel \(K\) with a layer \(I\) expressed as the product of a doubly blocked Toeplitz matrix derived from \(K\) with the column vector of the row-wise unrolled entries from \(I\).</p>
<p>The following python function is a simple implementation of this method</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">toeplitz</span>

<span class="k">def</span> <span class="nf">convolution</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="c1"># flip the kernel</span>
    <span class="n">K</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fliplr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flipud</span><span class="p">(</span><span class="n">K</span><span class="p">))</span>
    <span class="c1"># calculate sizes</span>
    <span class="n">K_row_num</span><span class="p">,</span> <span class="n">K_col_num</span><span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">I_row_num</span><span class="p">,</span> <span class="n">I_col_num</span><span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">R_row_num</span><span class="o">=</span> <span class="n">K_row_num</span><span class="o">+</span><span class="n">I_row_num</span><span class="o">-</span><span class="mi">1</span>
    <span class="n">R_col_num</span><span class="o">=</span> <span class="n">K_col_num</span><span class="o">+</span><span class="n">I_col_num</span><span class="o">-</span><span class="mi">1</span>
    <span class="c1"># pad the kernel</span>
    <span class="n">K_pad</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="n">R_row_num</span> <span class="o">-</span> <span class="n">K_row_num</span><span class="p">),</span>
                      <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">R_col_num</span> <span class="o">-</span> <span class="n">K_col_num</span><span class="p">)),</span> 
                  <span class="s1">'constant'</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"padded kernel= </span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">K_pad</span><span class="p">)</span>
    <span class="c1"># Assemble the list of Toeplitz matrices F_i</span>
    <span class="n">toeplitz_list</span><span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">R_row_num</span><span class="p">):</span>
        <span class="n">c</span><span class="o">=</span> <span class="n">K_pad</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span>
        <span class="n">r</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">I_col_num</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
        <span class="n">toeplitz_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">toeplitz</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">r</span><span class="p">)</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Toeplitz list= </span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">toeplitz_list</span><span class="p">)</span>
    <span class="c1"># make a matrix with the indices of the block F_i </span>
    <span class="c1"># of the doubly blocked Toeplitz matrix</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">R_row_num</span><span class="p">))</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="n">doubly_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">toeplitz</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">r</span><span class="p">)</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"doubly_indices= </span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">doubly_indices</span><span class="p">)</span>
    <span class="c1"># assemble the doubly blocked toeplitz matrix</span>
    <span class="n">toeplitz_m</span><span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">R_row_num</span><span class="p">):</span>
        <span class="n">row</span><span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">I_row_num</span><span class="p">):</span>
            <span class="n">row</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">toeplitz_list</span><span class="p">[</span><span class="n">doubly_indices</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]])</span>
        <span class="n">row</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
        <span class="n">toeplitz_m</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
    <span class="n">toeplitz_m</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">toeplitz_m</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Toeplitz matrix= </span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span><span class="n">toeplitz_m</span><span class="p">)</span>
    <span class="c1"># make layer into column vector</span>
    <span class="n">I_col</span><span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"I_col= "</span><span class="p">,</span> <span class="n">I_col</span><span class="p">)</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">toeplitz_m</span><span class="p">,</span> <span class="n">I_col</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'R as vector= </span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
    <span class="n">R</span><span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">R_row_num</span><span class="p">,</span> <span class="n">R_col_num</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span> 
        <span class="nb">print</span><span class="p">(</span><span class="s1">'R as matrix= </span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">R</span>
</pre></div>

<p>To test, one can, for instance, use</p>
<div class="code"><pre class="code literal-block"><span class="c1"># kernel</span>
<span class="n">K</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">],[</span><span class="mi">30</span><span class="p">,</span><span class="mi">40</span><span class="p">]])</span>
<span class="c1"># layer</span>
<span class="n">I</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">R</span><span class="o">=</span> <span class="n">convolution</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>

<p>The output would then be</p>
<div class="code"><pre class="code literal-block">padded kernel= 
 [[40 30  0  0]
 [20 10  0  0]
 [ 0  0  0  0]]
Toeplitz list= 
 [array([[40.,  0.,  0.],
       [30., 40.,  0.],
       [ 0., 30., 40.],
       [ 0.,  0., 30.]]), array([[20.,  0.,  0.],
       [10., 20.,  0.],
       [ 0., 10., 20.],
       [ 0.,  0., 10.]]), array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]])]
doubly_indices= 
 [[0 2]
 [1 0]
 [2 1]]
Toeplitz matrix= 
 [[40.  0.  0.  0.  0.  0.]
 [30. 40.  0.  0.  0.  0.]
 [ 0. 30. 40.  0.  0.  0.]
 [ 0.  0. 30.  0.  0.  0.]
 [20.  0.  0. 40.  0.  0.]
 [10. 20.  0. 30. 40.  0.]
 [ 0. 10. 20.  0. 30. 40.]
 [ 0.  0. 10.  0.  0. 30.]
 [ 0.  0.  0. 20.  0.  0.]
 [ 0.  0.  0. 10. 20.  0.]
 [ 0.  0.  0.  0. 10. 20.]
 [ 0.  0.  0.  0.  0. 10.]]
I_col=  [1 2 3 4 5 6]
R as vector= 
 [ 40. 110. 180.  90. 180. 370. 470. 210.  80. 140. 170.  60.]
R as matrix= 
 [[ 40. 110. 180.  90.]
 [180. 370. 470. 210.]
 [ 80. 140. 170.  60.]]
</pre></div>

<p>Note, that this example is inspired by <a href="https://raw.githubusercontent.com/alisaaalehi/convolution_as_multiplication/master/ConvAsMulExplained.pdf">Salehi's tutorial</a> but because we are calculating the machine learning covolution (cross-correlation) and Salehi the mathematical convolution as used in signal processing, the results are not the same. To generate identical results one can use the doubly flipped kernel,</p>
<div class="code"><pre class="code literal-block"><span class="c1"># kernel</span>
<span class="n">K</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">40</span><span class="p">,</span><span class="mi">30</span><span class="p">],[</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">]])</span>
<span class="c1"># layer</span>
<span class="n">I</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">R</span><span class="o">=</span> <span class="n">convolution</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"R= </span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
</pre></div>

<p>and obtain</p>
<div class="code"><pre class="code literal-block"> <span class="n">R</span><span class="o">=</span> 
 <span class="p">[[</span> <span class="mf">10.</span>  <span class="mf">40.</span>  <span class="mf">70.</span>  <span class="mf">60.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">70.</span> <span class="mf">230.</span> <span class="mf">330.</span> <span class="mf">240.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">120.</span> <span class="mf">310.</span> <span class="mf">380.</span> <span class="mf">240.</span><span class="p">]]</span>
</pre></div>

<p>which exactly is Salehi's result.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>Convolution images created with software from:
Vincent Dumoulin and Francesco Visin, A guide to convolution arithmetic for deep learning (2016) ArXiv e-prints 1603.07285; <a href="https://github.com/vdumoulin/conv_arithmetic">Software on github</a> <a class="footnote-backref" href="../posts/sw_blog_toeplitz.html#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
    </div>
    </article>
</div>

    

                <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
        </script>
</div>
            <script src="../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
