<!DOCTYPE html>
<html prefix="
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Welcome to GeNN">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>GeNN · Software Developer Blog </title>
<link href="../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../rss.xml">
<link rel="canonical" href="http://genn-team.github.io/blog/">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]-->
</head>
<body class="">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="hsidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="http://genn-team.github.io/">
                      <h1 id="brand"><a href="http://genn-team.github.io/" title="GeNN" rel="home">

        <span class="blog-title" id="blog-title">GeNN</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead">Welcome to GeNN</p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="https://genn-team.github.io/genn/documentation/4/html/index.html">Documentation</a>
        <a class="sidebar-nav-item" href=".">Latest Blog</a>
        <a class="sidebar-nav-item" href="../archive.html">Blog Archive</a>
        <a class="sidebar-nav-item" href="../categories/">Blog Tags</a>
        <a class="sidebar-nav-item" href="../rss.xml">Blog RSS feed</a>
    
    
    </nav><footer id="footer"><span class="copyright">
              Contents © 2021         <a href="mailto:t.nowotny@sussex.ac.uk">GeNN Team</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            </span>
            
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
    
<div class="post">
    <article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="../posts/sw_blog_toeplitz.html" class="u-url">Software Developer Blog: How to do convolutions with doubly blocked Toeplitz matrices</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2021-12-21T14:39:44Z" title="2021-12-21 14:39">2021-12-21 14:39</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h2>How to do convolutions with doubly blocked Toeplitz matrices</h2>
<p>A few weeks ago, Jamie (@neworderofjamie) asked me on the chat whether I knew what doubly blocked Toeplitz matrices are and how they implement convolutions. I had no clue. Since then we have implemented convolutions using doubly blocked Toeplitz matrices in GeNN and found them to be extremely useful and efficient.
1
In this software blog I will give a brief overview on the why and how convolutions relate to doubly blocked Toeplitz matrices. My blog is based on Ali Salehi's tutorial <a href="../posts/%7Bhttps:/raw.githubusercontent.com/alisaaalehi/convolution_as_multiplication/master/ConvAsMulExplained.pdf">Convolution as Matrix Multiplication</a>  but updated to use machine-learning rather than signal-processing conventions and I am trying to avoid using too many unusual ways of re-arranging rows and columns.</p>
<h3>The why</h3>
<p>Let us consider the convolution of a \(2\times 2\) kernel with a \(3\times 3\) layer. We denote the kernel as
\[
K= \left(\matrix{
k_{11} &amp; k_{12} \cr
k_{21} &amp; k_{22}}\right)
\]
and the layer as
\[
I= \left(\matrix{
i_{11} &amp; i_{12} &amp; i_{13} \cr
i_{21} &amp; i_{22} &amp; i_{23} \cr
i_{31} &amp; i_{32} &amp; i_{33}
} \right).
\]
Then the convolution in the machine learning use of the term is calculating the cross-correlation of the kernel "moving across" the layer as illustrated below. The layer \(I\) is in blue, the kernel \(K\) in grey and the result \(R\) in green.</p>
<table>
<thead><tr>
<th align="center">
<sup id="fnref:1"><a class="footnote-ref" href="../posts/sw_blog_toeplitz.html#fn:1">1</a></sup><img alt="Illustration of convolution step" src="../images/blog_00.png">
</th>
<th align="center"><img alt="Illustration of convolution step" src="../images/blog_01.png"></th>
<th align="center"><img alt="Illustration of convolution step" src="../images/blog_02.png"></th>
<th align="center"><img alt="Illustration of convolution step" src="../images/blog_03.png"></th>
</tr></thead>
<tbody><tr>
<td align="center">\(r_{11}\)</td>
<td align="center">\(r_{12}\)</td>
<td align="center">\(r_{13}\)</td>
<td align="center">\(3_{14}\)</td>
</tr></tbody>
</table>
<p>For the first non-zero entry at \((1,1)\) of the result matrix \(R\), we therefore have \(r_{11} = k_{22} i_{11}\).
Then the kernel moves one over and \(r_{12} = k_{21}i_{11} + k_{22} i_{12}\). Then, \(r_{13} = k_{21}i_{12} + k_{22} i_{13}\) and \(r_{14} = k_{21}i_{13} \).</p>
<table>
<thead><tr>
<th align="center"><img alt="Illustration of convolution step" src="../images/blog_04.png"></th>
<th align="center"><img alt="Illustration of convolution step" src="../images/blog_05.png"></th>
<th align="center"><img alt="Illustration of convolution step" src="../images/blog_06.png"></th>
<th align="center"><img alt="Illustration of convolution step" src="../images/blog_07.png"></th>
</tr></thead>
<tbody><tr>
<td align="center">\(r_{21}\)</td>
<td align="center">\(r_{22}\)</td>
<td align="center">\(r_{23}\)</td>
<td align="center">\(r_{24}\)</td>
</tr></tbody>
</table>
<p>So, for the second row,
\(r_{21} = k_{12} i_{11} + k_{22} i_{21} \), move one over,
\(r_{22} = k_{11} i_{11} + k_{12} i_{12} + k_{21} i_{21} + k_{22} i_{22} \), one more to the right,
\(r_{23} = k_{11}i_{12} + k_{12} i_{13} + k_{21} i_{22} + k_{22} i_{23} \), and finally
\(r_{24} = k_{11}i_{13} + k_{21} i_{23} \).</p>
<p>It works similar for the remaining two rows.</p>
<p>If we unroll the layer \(I\) row-wise into a column vector \(I_\text{col}\),
\[
    I_\text{col} = 
    \left(
    \matrix{ 
        i_{11} \cr
        i_{12} \cr
        i_{13} \cr
        i_{21} \cr
        i_{22} \cr
        i_{23} \cr
        i_{31} \cr
        i_{32} \cr
        i_{33}
        }
    \right),
\]
then we can express this as a matrix-vector multiplication of a matrix formed from the entries of the kernel \(K\) and the vector\(I_\text{col}\), 
\[
\left(\matrix{
k_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cr
k_{21} &amp; k_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cr
0 &amp; k_{21} &amp; k_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cr
0 &amp; 0 &amp; k_{21} &amp; k_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cr
k_{12} &amp; 0 &amp; 0 &amp; k_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cr
k_{11} &amp; k_{12} &amp; 0 &amp; k_{21} &amp; k_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cr
0 &amp; k_{11} &amp; k_{12} &amp; 0 &amp; k_{21} &amp; k_{22} &amp; 0 &amp; 0 &amp; 0 \cr
0 &amp; 0 &amp; k_{11} &amp; 0 &amp; 0 &amp; k_{21} &amp; 0 &amp; 0 &amp; 0 \cr
0 &amp; 0 &amp; 0 &amp; k_{12} &amp; 0 &amp; 0 &amp; k_{22} &amp; 0 &amp; 0 \cr
0 &amp; 0 &amp; 0 &amp; k_{11} &amp; k_{12} &amp; 0 &amp; k_{21} &amp; k_{22} &amp; 0 \cr
0 &amp; 0 &amp; 0 &amp; 0 &amp; k_{11} &amp; k_{12} &amp; 0 &amp; k_{21} &amp; k_{22} \cr
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; k_{11} &amp; 0 &amp; 0 &amp; k_{21} \cr
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; k_{12} &amp; 0 &amp; 0 \cr
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; k_{11} &amp; k_{12} &amp; 0 \cr
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; k_{11} &amp; k_{12} \cr
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; k_{11} }\right)
\cdot
\left(\matrix{
i_{11} \cr
i_{12} \cr
i_{13} \cr
i_{21} \cr
i_{22} \cr
i_{23} \cr
i_{31} \cr
i_{32} \cr
i_{33}} 
\right)
\]</p>
<p>Now one can already see that the matrix formed from the kernel entries has a very peculiar shape - the shape of a doubly blocked Toeplitz matrix</p>
<h3>Doubly blocked Toeplitz matrix</h3>
<p>A Toeplitz matrix is a matrix where the values along all diagonals are constant, i.e.</p>
<p>\[
\left(
    \matrix{ 
        a_{0} &amp; a_{-1} &amp; a_{-2} &amp; \cdots  &amp; \cdots &amp; \cdots &amp; a_{-(N-1)} \cr
        a_{1} &amp; a_{0} &amp; a_{-1} &amp; a_{-2} &amp;  &amp; &amp; \vdots \cr
        a_{2} &amp; a_{1} &amp; a_{0} &amp; a_{-1} &amp;  &amp; &amp; \vdots \cr
        \vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; &amp; \vdots \cr
        \vdots &amp; &amp; &amp; \ddots  &amp; a_{0} &amp; a_{-1} &amp; a_{-2} \cr
        \vdots &amp; &amp; &amp;  &amp; a_{1} &amp; a_{0} &amp; a_{-1} \cr
        a_{M-1} &amp; \cdots  &amp; \cdots &amp; \cdots &amp; a_{2} &amp; a_{1} &amp; a_{0} }
    \right) .
\]</p>
<p>Furthermore, if we build a matrix \(A\) out of Toeplitz sub-matrices \(A_{k}\) <em>and</em> the structure of \(A\) with respect to these submatrices is also Toeplitz:</p>
<p>\[
    A = \left(
    \matrix{ 
        A_{0} &amp; A_{-1} &amp; \cdots &amp; A_{-(L-1)} \cr
        A_{1} &amp; A_{0} &amp; \cdots &amp; A_{-(L-2)} \cr
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \cr
        A_{K} &amp; A_{K-1} &amp; \cdots &amp; A_{0}}
    \right),
\]</p>
<p>then, this matrix is called a doubly-blocked Toeplitz matrix. A standard way to generate a Toeplitz matrix from a vector \(v\) is to use \(v\) as the first column vector, then make one cyclic permutation and use it as the second column vector and so on.</p>
<h3>The method</h3>
<p>As we have seen on the example above, 2D convolution operations can be expressed as multiplication by a doubly-blocked Toeplitz matrix. As a general method, applied to the example above,
to convolve \(K\) with \(I\), we first flip \(K\) across the horizontal and vertical axis and pad it to the output size \((I_\text{height} + K_\text{height} - 1) \times (I_\text{width} + K_\text{width} - 1)\) of the convolution.
For instance, here, the \(3 \times 3\) layer \(I\) covolved by \(K\) above, leads to output size \(4 \times 4\).
Depending on the padding mode used by the convolution, typically, only part of this output is actually required.
The flipped and padded kernel \(K\) from above is
\[
    K_\text{pad}=
    \left(
    \matrix{ 
        k_{22} &amp; k_{21} &amp; 0 &amp; 0 \cr
        k_{12} &amp; k_{11} &amp; 0 &amp; 0 \cr
        0 &amp; 0 &amp; 0 &amp; 0 \cr
        0 &amp; 0 &amp; 0 &amp; 0 }
    \right)
\]</p>
<p>We then convert each <em>row vector</em> of this matrix into Toeplitz matrices \(F_i\) as described above:
\[
    F_0=
    \left(
    \matrix{ 
        k_{22} &amp; 0 &amp; 0 \cr
        k_{21} &amp; k_{22} &amp; 0 \cr
        0 &amp; k_{21} &amp; k_{22} \cr
        0 &amp; 0 &amp; k_{21}}
    \right)
    \quad
    F_1=
    \left(
    \matrix{ 
        k_{12} &amp; 0 &amp;  0 \cr
        k_{11} &amp; k_{12} &amp; 0 \cr
        0 &amp;  k_{11} &amp; k_{12} \cr
        0 &amp;  0 &amp;  k_{11}}
    \right)
    \]
    \[
    F_2=
    \left(
    \matrix{ 
        0 &amp; 0  &amp; 0 \cr
        0 &amp; 0 &amp; 0 \cr
        0  &amp; 0 &amp; 0 \cr
        0  &amp; 0  &amp; 0}
    \right)
    \quad
    F_3=
    \left(
    \matrix{ 
        0 &amp; 0  &amp; 0 \cr
        0 &amp; 0 &amp; 0 \cr
        0  &amp; 0 &amp; 0 \cr
        0  &amp; 0  &amp; 0}
    \right)
\]
and, finally, assemble these into a doubly blocked Toeplitz matrix \(F\):</p>
<p>\[
    F=
    \left(
    \matrix{ 
        F_0 &amp; F_3 &amp; F_2 \cr
        F_1 &amp; F_0 &amp; F_3 \cr
        F_2 &amp; F_1 &amp; F_0 \cr
        F_3 &amp; F_2 &amp; F_1
    }
    \right)
\]</p>
<p>The convolution of \(K\) with \(I\)
is then given by multiplying F from the left onto \(I_\text{col}\) as defined above,
\[
  R_{\text{col}} = F \cdot I  \quad 
  \Leftrightarrow \quad R_{\text{col},j}= \sum_i F_{ji}I_i 
  \]</p>
<p>Finally, \(R_{\text{col}}\) can be reinterpreted as the output matrix \(R\) by arranging its entries row-wise in a \(4\times 4\) matrix.</p>
<p>There we have it - convolution (in the machine learning sense, i.e. corss-correlation) of a kernel \(K\) with a layer \(I\) expressed as the product of a doubly blocked Toeplitz matrix derived from \(K\) with the column vector of the row-wise unrolled entries from \(I\).</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>Convolution images created with software from:
Vincent Dumoulin and Francesco Visin, A guide to convolution arithmetic for deep learning (2016) ArXiv e-prints 1603.07285; <a href="https://github.com/vdumoulin/conv_arithmetic">Software on github</a> <a class="footnote-backref" href="../posts/sw_blog_toeplitz.html#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
</div>
    </div>
    </article>
</div>


                <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
        </script>
</div>
            <script src="../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
